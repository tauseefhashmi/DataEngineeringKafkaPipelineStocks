{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MasterThesisProject.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "mount_file_id": "1m-FU37SPI0MCcvtNAMTwMavKa7IJ1nRg",
      "authorship_tag": "ABX9TyMtU9/L/B2/YDXQFfK5OzMj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tauseefhashmi/DataEngineeringKafkaPipelineStocks/blob/main/MasterThesisProject.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nEM3MiD4gA9W",
        "outputId": "40d5b69a-bb57-4874-e753-252f93e64dc0"
      },
      "source": [
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (1.24.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WuRMskk2f3XE"
      },
      "source": [
        "import pandas as pd\n",
        "import urllib\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "url=\"https://bbooks.info/b/w/1f7fb81b4cb7171fbba0702a6f1702d31c198c1f/wild-sands.pdf\"\n",
        "res=requests.get(url)\n",
        "html_page=res.content\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9qeloOzLApcB"
      },
      "source": [
        "soup=BeautifulSoup(html_page,features='html.parser')\n",
        "text=soup.find_all(text=True)\n",
        "lines=[line2 for line in text for line2 in line.strip().split('.') if line2]\n",
        "lines"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A3NIMj2oGLh2"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jj74NhEKDHDE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ecb824e3-9df5-4dcc-b45b-1899c6946e98"
      },
      "source": [
        "!pip install word2vec"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting word2vec\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/11/9e/dc6d96578191b6167cb1ea4a3fe3edeed0dce54d3db21ada013b2b407d65/word2vec-0.11.1.tar.gz (42kB)\n",
            "\r\u001b[K     |███████▊                        | 10kB 13.3MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 20kB 9.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 30kB 5.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 40kB 5.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 51kB 2.1MB/s \n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.9.2 in /usr/local/lib/python3.7/dist-packages (from word2vec) (1.19.5)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from word2vec) (1.0.1)\n",
            "Building wheels for collected packages: word2vec\n",
            "  Building wheel for word2vec (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for word2vec: filename=word2vec-0.11.1-cp37-none-any.whl size=156413 sha256=cc25e9932e5d20763c0351d60c6722ce402ec948260e056b9f9c78e826298167\n",
            "  Stored in directory: /root/.cache/pip/wheels/f3/7c/ac/fcb6d867f806021c3730fd848970db988b1d0030b5d20c0e02\n",
            "Successfully built word2vec\n",
            "Installing collected packages: word2vec\n",
            "Successfully installed word2vec-0.11.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MrBw37ytcY8U"
      },
      "source": [
        "!pip install nltk\n",
        "!python -m nltk.downloader all\n",
        "!pip install unidecode\n",
        "!pip install sklearn\n",
        "!pip install gensim\n",
        "!pip install numpy\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "mx2O3DnUb6tI",
        "outputId": "a1b4af9d-c189-4342-8bb3-b877e4cbc5ea"
      },
      "source": [
        "##Pre-Processing\n",
        "import nltk\n",
        "from nltk import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from unidecode import unidecode\n",
        "import string\n",
        "def pre_process(corpus):\n",
        "    # convert input corpus to lower case.\n",
        "    corpus = corpus.lower()\n",
        "    # collecting a list of stop words from nltk and punctuation form\n",
        "    # string class and create single array.\n",
        "    stopset = stopwords.words('english') + list(string.punctuation)\n",
        "    # remove stop words and punctuations from string.\n",
        "    # word_tokenize is used to tokenize the input corpus in word tokens.\n",
        "    corpus = \" \".join([i for i in word_tokenize(corpus) if i not in stopset])\n",
        "    # remove non-ascii characters\n",
        "    corpus = unidecode(corpus)\n",
        "    return corpus\n",
        "pre_process(\"Sample of non ASCII: Ceñía. How to remove stopwords and punctuations?\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'sample non ascii cenia remove stopwords punctuations'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VNb8XpA_lWVS",
        "outputId": "cc5b2611-d3dc-4aac-ec2a-805459882491"
      },
      "source": [
        "##Lemmitization\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "sentence = \"The striped bats are hanging on their feet for best\"\n",
        "words = word_tokenize(sentence)\n",
        "for w in words:\n",
        "  print(w, \" : \", lemmatizer.lemmatize(w))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The  :  The\n",
            "striped  :  striped\n",
            "bats  :  bat\n",
            "are  :  are\n",
            "hanging  :  hanging\n",
            "on  :  on\n",
            "their  :  their\n",
            "feet  :  foot\n",
            "for  :  for\n",
            "best  :  best\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NeopaT6Hn8Cd"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "# sentence pair\n",
        "corpus = [\"A girl is styling her hair.\", \"A girl is brushing her hair.\"]\n",
        "for c in range(len(corpus)):\n",
        "    corpus[c] = pre_process(corpus[c])\n",
        "# creating vocabulary using uni-gram and bi-gram\n",
        "tfidf_vectorizer = TfidfVectorizer(ngram_range=(1,2))\n",
        "tfidf_vectorizer.fit(corpus)\n",
        "feature_vectors = tfidf_vectorizer.transform(corpus)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7qBkpOvTdqoo"
      },
      "source": [
        "from gensim.models import Word2Vec\n",
        "import numpy as np\n",
        "# give a path of model to load function\n",
        "word_emb_model = Word2Vec.load('word2vec.bin')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2p2rp1bwiWyk",
        "outputId": "2d707958-37a2-4a2a-b618-6572ee7906fc"
      },
      "source": [
        "EMBEDDING_FILE = '/root/input/GoogleNews-vectors-negative300.bin.gz'\n",
        "!wget -P /root/input/ -c \"https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-06-01 21:58:52--  https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.217.44.182\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.217.44.182|:443... connected.\n",
            "HTTP request sent, awaiting response... 416 Requested Range Not Satisfiable\n",
            "\n",
            "    The file is already fully retrieved; nothing to do.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MVI1rATnkjTG"
      },
      "source": [
        "from gensim.models import Word2Vec\n",
        "import numpy as np\n",
        "# give a path of model to load function\n",
        "#word_emb_model = Word2Vec.load(EMBEDDING_FILE)\n",
        "Word2Vec.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True,norm_only=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-cY8PfMpmbbE"
      },
      "source": [
        "from gensim.models import Word2Vec\n",
        "import numpy as np\n",
        "from gensim.models.keyedvectors import KeyedVectors\n",
        "# give a path of model to load function\n",
        "#word_emb_model = Word2Vec.load_word2vec_format('./GoogleNews-vectors-negative300.bin', binary=True)\n",
        "word_emb_model = KeyedVectors.load_word2vec_format('https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz', binary=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YDnDga6Lk3hc"
      },
      "source": [
        "from collections import Counter\n",
        "import itertools\n",
        "\n",
        "def map_word_frequency(document):\n",
        "    return Counter(itertools.chain(*document))\n",
        "    \n",
        "def get_sif_feature_vectors(sentence1, sentence2, word_emb_model=word_emb_model):\n",
        "    sentence1 = [token for token in sentence1.split() if token in word_emb_model.wv.vocab]\n",
        "    sentence2 = [token for token in sentence2.split() if token in word_emb_model.wv.vocab]\n",
        "    word_counts = map_word_frequency((sentence1 + sentence2))\n",
        "    embedding_size = 300 # size of vectore in word embeddings\n",
        "    a = 0.001\n",
        "    sentence_set=[]\n",
        "    for sentence in [sentence1, sentence2]:\n",
        "        vs = np.zeros(embedding_size)\n",
        "        sentence_length = len(sentence)\n",
        "        for word in sentence:\n",
        "            a_value = a / (a + word_counts[word]) # smooth inverse frequency, SIF\n",
        "            vs = np.add(vs, np.multiply(a_value, word_emb_model.wv[word])) # vs += sif * word_vector\n",
        "        vs = np.divide(vs, sentence_length) # weighted average\n",
        "        sentence_set.append(vs)\n",
        "    return sentence_set"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wmEa71iTlBer"
      },
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "def get_cosine_similarity(feature_vec_1, feature_vec_2):    \n",
        "    return cosine_similarity(feature_vec_1.reshape(1, -1), feature_vec_2.reshape(1, -1))[0][0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nDA6ao5Knsk1"
      },
      "source": [
        "## **PIPELINE**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cCxbBhpOn2rx",
        "outputId": "60dbfe7d-3030-489f-e882-f0498e5c1595"
      },
      "source": [
        "!pip install -U gensim"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting gensim\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/44/52/f1417772965652d4ca6f901515debcd9d6c5430969e8c02ee7737e6de61c/gensim-4.0.1-cp37-cp37m-manylinux1_x86_64.whl (23.9MB)\n",
            "\u001b[K     |████████████████████████████████| 23.9MB 1.9MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: smart-open>=1.8.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (5.0.0)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.11.3 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.19.5)\n",
            "Requirement already satisfied, skipping upgrade: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.4.1)\n",
            "Installing collected packages: gensim\n",
            "  Found existing installation: gensim 3.6.0\n",
            "    Uninstalling gensim-3.6.0:\n",
            "      Successfully uninstalled gensim-3.6.0\n",
            "Successfully installed gensim-4.0.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 396
        },
        "id": "JhER-AJj-ts_",
        "outputId": "684db913-2c81-4f95-e229-6c0739c9eec1"
      },
      "source": [
        "#python example to train doc2vec model (with or without pre-trained word embeddings)\n",
        "\n",
        "import gensim.models as g\n",
        "import logging\n",
        "\n",
        "#doc2vec parameters\n",
        "vector_size = 300\n",
        "window_size = 15\n",
        "min_count = 1\n",
        "sampling_threshold = 1e-5\n",
        "negative_size = 5\n",
        "train_epoch = 100\n",
        "dm = 0 #0 = dbow; 1 = dmpv\n",
        "worker_count = 1 #number of parallel processes\n",
        "\n",
        "#pretrained word embeddings\n",
        "pretrained_emb = \"toy_data/pretrained_word_embeddings.txt\" #None if use without pretrained embeddings\n",
        "\n",
        "#input corpus\n",
        "train_corpus = \"toy_data/train_docs.txt\"\n",
        "\n",
        "#output model\n",
        "saved_path = \"toy_data/model.bin\"\n",
        "\n",
        "#enable logging\n",
        "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
        "\n",
        "#train doc2vec model\n",
        "docs = g.doc2vec.TaggedLineDocument(train_corpus)\n",
        "model = g.Doc2Vec(docs, size=vector_size, window=window_size, min_count=min_count, sample=sampling_threshold, workers=worker_count, hs=0, dm=dm, negative=negative_size, dbow_words=1, dm_concat=1, pretrained_emb=pretrained_emb, iter=train_epoch)\n",
        "\n",
        "#save model\n",
        "model.save(saved_path)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-623952db33f2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;31m#train doc2vec model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0mdocs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdoc2vec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTaggedLineDocument\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_corpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDoc2Vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvector_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwindow_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmin_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msampling_threshold\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworker_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnegative\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnegative_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdbow_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdm_concat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpretrained_emb\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpretrained_emb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;31m#save model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gensim/models/doc2vec.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, documents, corpus_file, vector_size, dm_mean, dm, dbow_words, dm_concat, dm_tag_count, dv, dv_mapfile, comment, trim_rule, callbacks, window, epochs, **kwargs)\u001b[0m\n\u001b[1;32m    294\u001b[0m             \u001b[0mwindow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m             \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m             \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m         )\n\u001b[1;32m    298\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'size'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mZ7exkr-9Jt6"
      },
      "source": [
        "#python example to infer document vectors from trained doc2vec model\n",
        "import gensim.models as g\n",
        "import codecs\n",
        "\n",
        "#parameters\n",
        "model=\"toy_data/model.bin\"\n",
        "test_docs=\"toy_data/test_docs.txt\"\n",
        "output_file=\"toy_data/test_vectors.txt\"\n",
        "\n",
        "#inference hyper-parameters\n",
        "start_alpha=0.01\n",
        "infer_epoch=1000\n",
        "\n",
        "#load model\n",
        "m = g.Doc2Vec.load(model)\n",
        "test_docs = [ x.strip().split() for x in codecs.open(test_docs, \"r\", \"utf-8\").readlines() ]\n",
        "\n",
        "#infer test vectors\n",
        "output = open(output_file, \"w\")\n",
        "for d in test_docs:\n",
        "    output.write( \" \".join([str(x) for x in m.infer_vector(d, alpha=start_alpha, steps=infer_epoch)]) + \"\\n\" )\n",
        "output.flush()\n",
        "output.close()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fibVHc95FbZN"
      },
      "source": [
        "### PIPELINE 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X2tdULP3FbG8"
      },
      "source": [
        "from gensim.test.utils import common_texts\n",
        "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
        "\n",
        "documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(common_texts)]\n",
        "model = Doc2Vec(documents, vector_size=5, window=2, min_count=1, workers=4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "75o-mlQoFxYO"
      },
      "source": [
        "from gensim.test.utils import get_tmpfile\n",
        "fname = get_tmpfile(\"my_doc2vec_model\")\n",
        "\n",
        "model.save(fname)\n",
        "model = Doc2Vec.load(fname)  # you can continue training with the loaded model!"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AD3sOZG_F5wM"
      },
      "source": [
        "vector = model.infer_vector([\"system\", \"response\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fIs2AUJBGf3p"
      },
      "source": [
        "###PIPELINE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8kTazOFHGlrm",
        "outputId": "7e749c20-7aab-4209-ad06-2fefbe299dad"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "corpus = [\"I'd like an apple\",\"An apple a day keeps the doctor away\",\"Never compare an apple to an orange\",\"I prefer scikit-learn to Orange\",\"The scikit-learn docs are Orange and Blue\"]                                                                                                                                                                                                   \n",
        "vect = TfidfVectorizer(min_df=1, stop_words=\"english\")                                                                                                                                                                                                   \n",
        "tfidf = vect.fit_transform(corpus)                                                                                                                                                                                                                       \n",
        "pairwise_similarity = tfidf * tfidf.T\n",
        "print(pairwise_similarity)\n",
        "print(pairwise_similarity.toarray())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  (0, 2)\t0.27056873300683837\n",
            "  (0, 1)\t0.17668795478716204\n",
            "  (0, 0)\t0.9999999999999998\n",
            "  (1, 2)\t0.1543943648960287\n",
            "  (1, 0)\t0.17668795478716204\n",
            "  (1, 1)\t0.9999999999999999\n",
            "  (2, 1)\t0.1543943648960287\n",
            "  (2, 0)\t0.27056873300683837\n",
            "  (2, 4)\t0.16815247007633355\n",
            "  (2, 3)\t0.1963564882520361\n",
            "  (2, 2)\t1.0\n",
            "  (3, 2)\t0.1963564882520361\n",
            "  (3, 4)\t0.5449975578692606\n",
            "  (3, 3)\t0.9999999999999999\n",
            "  (4, 2)\t0.16815247007633355\n",
            "  (4, 3)\t0.5449975578692606\n",
            "  (4, 4)\t1.0\n",
            "[[1.         0.17668795 0.27056873 0.         0.        ]\n",
            " [0.17668795 1.         0.15439436 0.         0.        ]\n",
            " [0.27056873 0.15439436 1.         0.19635649 0.16815247]\n",
            " [0.         0.         0.19635649 1.         0.54499756]\n",
            " [0.         0.         0.16815247 0.54499756 1.        ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "esgf2CUoHfaH"
      },
      "source": [
        "###PIPELINE 4"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 571
        },
        "id": "-iHIMKMOHho1",
        "outputId": "e8de7333-9b4c-4fc3-900a-fc8face6e585"
      },
      "source": [
        "import nltk, string\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "nltk.download('punkt') \n",
        "!pip install python-docx\n",
        "from docx import Document\n",
        "text_sample1=[]\n",
        "document = Document('/content/sample_data/BullDog.docx')\n",
        "for para in document.paragraphs:\n",
        "    print(para.text)\n",
        "    text_sample1.append(para.text)\n",
        "text_sample2=[]\n",
        "document = Document('/content/sample_data/GoldenRetriever.docx')\n",
        "for para in document.paragraphs:\n",
        "    print(para.text)\n",
        "    text_sample2.append(para.text)\n",
        "text_sample3=[]\n",
        "document = Document('/content/sample_data/Labrador.docx')\n",
        "for para in document.paragraphs:\n",
        "    print(para.text)\n",
        "    text_sample3.append(para.text)    \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "stemmer = nltk.stem.porter.PorterStemmer()\n",
        "remove_punctuation_map = dict((ord(char), None) for char in string.punctuation)\n",
        "\n",
        "def stem_tokens(tokens):\n",
        "    return [stemmer.stem(item) for item in tokens]\n",
        "\n",
        "'''remove punctuation, lowercase, stem'''\n",
        "def normalize(text):\n",
        "    return stem_tokens(nltk.word_tokenize(text.lower().translate(remove_punctuation_map)))\n",
        "\n",
        "vectorizer = TfidfVectorizer(tokenizer=normalize, stop_words='english')\n",
        "\n",
        "def cosine_sim(text1, text2):\n",
        "    tfidf = vectorizer.fit_transform([text1, text2])\n",
        "    return ((tfidf * tfidf.T).A)[0,1]\n",
        "\n",
        "\n",
        "print(cosine_sim('a little bird', 'a little bird'))\n",
        "print(cosine_sim('a little bird', 'a little bird chirps'))\n",
        "print(cosine_sim('a little bird', 'a big dog barks'))\n",
        "\"\"\"print(cosine_sim(text_sample1, text_sample2))\n",
        "print(cosine_sim(text_sample1, text_sample3))\n",
        "print(cosine_sim(text_sample2, text_sample3))\"\"\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "Requirement already satisfied: python-docx in /usr/local/lib/python3.7/dist-packages (0.8.11)\n",
            "Requirement already satisfied: lxml>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from python-docx) (4.2.6)\n",
            "According to the American Kennel Club (AKC), a Bulldog's disposition should be \"equable and kind, resolute, and courageous (not vicious or aggressive), and demeanor should be pacifist and dignified. These attributes should be countenanced by the expression and behavior\".\n",
            "Breeders have worked to reduce/remove aggression from these dogs. Most have a friendly, patient, but stubborn nature. Bulldogs are recognized as excellent family pets because of their tendency to form strong bonds with children.\n",
            "Generally, Bulldogs are known for getting along well with children, other dogs, and other pets.\n",
            "The temperament of the Golden Retriever is a hallmark of the breed, and is described in the standard as \"kindly, friendly and confident\".Golden Retrievers make good family pets, particularly as they are patient with children.They are not \"one-man dogs\" and are generally equally amiable with both strangers and those familiar to them.Their trusting, gentle disposition makes them a poor guard dog. Any form of unprovoked aggression or hostility towards either people, dogs or other animals, whether in the show ring or community, is considered unacceptable in a Golden Retriever and is not in keeping with the character of the breed, nor should a Golden Retriever be unduly timid or nervous.The typical Golden Retriever is calm, naturally intelligent and biddable, and with an exceptional eagerness to please.\n",
            "\n",
            "Golden Retrievers are also noted for their intelligence. The breed ranks fourth in Stanley Coren's The Intelligence of Dogs – following the Border Collie, Poodle, and German Shepherd – as one of the brightest dogs ranked by obedience-command trainability.\n",
            "\n",
            "Typical Golden Retrievers are active and fun-loving animals with the exceptionally patient demeanour befitting a dog bred to sit quietly for hours in a hunting blind. Adult Goldens love to work, and have a keen ability to focus on a given task. They will work until they collapse, so care should be taken to avoid overworking them.\n",
            "\n",
            "Other characteristics related to their hunting heritage are a size suited for scrambling in and out of boats and an inordinate love for water. Golden Retrievers are exceptionally trainable—due to their intelligence, athleticism and desire to please their handlers—and excel in obedience trials. They are also very competitive in agility and other performance events. Harsh training methods are unnecessary, as Golden Retrievers often respond very well to positive and upbeat training styles.\n",
            "\n",
            "Golden Retrievers are compatible with other dogs, cats, and most livestock. They are particularly valued for their high level of sociability towards people, calmness, and willingness to learn. Because of this, they are commonly used as guide dogs, mobility assistance dogs, and search and rescue dogs.\n",
            "The AKC describes the Labrador's temperament as a kind, pleasant, outgoing and tractable nature.[8] Labradors' sense of smell allows them to home in on almost any scent and follow the path of its origin. They generally stay on the scent until they find it. Navies, military forces and police forces use them as detection dogs to track down smugglers, thieves, terrorists and black marketers. They are known to have a very soft feel to the mouth as a result of being bred to retrieve game such as waterfowl. They are prone to chewing objects (though they can be trained to abandon this behaviour).\n",
            "\n",
            "Labradors have a reputation as a very even-tempered breed and an excellent family dog.[8] This includes a good reputation with children of all ages and other animals. Some lines, particularly those that have continued to be bred specifically for their skills at working in the field (rather than for their appearance), are particularly fast and athletic. Their fun-loving boisterousness and lack of fear may require training and intelligent handling at times to ensure it does not get out of hand—an uncontrolled adult can be quite problematic. Bitches may be slightly more independent than dogs. Labradors mature at around three years of age; before this time they can have a significant degree of puppy-like energy, often mislabelled as being hyperactive. Because of their enthusiasm, leash-training early on is suggested to prevent pulling when full-grown. Labradors often enjoy retrieving a ball endlessly (often obsessively) and other forms of activity (such as agility, frisbee, or flyball).\n",
            "Although they will sometimes bark at noise, especially noise from an unseen source (\"alarm barking\"), Labradors are usually not noisy or territorial. They are often very easy-going and trusting with strangers and therefore are not usually suitable as guard dogs.\n",
            "\n",
            "Labradors as a breed are curious and exploratory and love company, following both people and interesting scents for food, attention, and novelty value. In this way, they can often \"vanish\" or otherwise become separated from their owners with little fanfare. As a breed, they are highly intelligent and capable of intense single-mindedness and focus if motivated or their interest is caught. Therefore, with the right conditions and stimuli, a bored Labrador could \"turn into an escape artist par excellence\". Many dogs are also stolen. Because of their curious nature and ability to \"vanish,\" along with the risk of being stolen, a number of dog clubs and rescue organisations (including the UK's Kennel Club) consider it good practice that Labradors be microchipped, with the owner's name and address also on their collar and tags.\n",
            "\n",
            "The steady temperament of Labradors and their ability to learn make them an ideal breed for search and rescue, detection, and therapy work. They are a very intelligent breed. They are ranked No. 7 in Stanley Coren's The Intelligence of Dogs, making it one of the brightest dogs out of 138 breeds tested. The AKC describes the breed as an ideal family and sporting dog. Their primary working role in the field continues to be that of a hunting retriever.\n",
            "0.9999999999999998\n",
            "0.7092972666062738\n",
            "0.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
            "  'stop_words.' % sorted(inconsistent))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'print(cosine_sim(text_sample1, text_sample2))\\nprint(cosine_sim(text_sample1, text_sample3))\\nprint(cosine_sim(text_sample2, text_sample3))'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JqRSYUcQGtZ7",
        "outputId": "2a0753dc-14e5-4bd7-adf5-901462b16196"
      },
      "source": [
        "print(text_sample1)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jkwWiznxIKg0",
        "outputId": "dfeaa2e4-15fc-48b1-c8bb-1653666cc4b4"
      },
      "source": [
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "doc1 = nlp(u'Hello hi there!')\n",
        "doc2 = nlp(u'Hello hi there!')\n",
        "doc3 = nlp(u'Hey whatsup?')\n",
        "\n",
        "print (doc1.similarity(doc2))\n",
        "print (doc2.similarity(doc3))\n",
        "print (doc1.similarity(doc3))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.0\n",
            "0.5156783284503755\n",
            "0.5156783284503755\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/lib/python3.7/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.7/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LnB5ztyuhGLO",
        "outputId": "c7289bde-5e6a-42a7-ab5f-82d5e3a1792c"
      },
      "source": [
        "!pip install python-docx\n",
        "from docx import Document\n",
        "text_sample1=[]\n",
        "document = Document('/content/sample_data/BullDog.docx')\n",
        "for para in document.paragraphs:\n",
        "    print(para.text)\n",
        "    text_sample1.append(para.text)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: python-docx in /usr/local/lib/python3.7/dist-packages (0.8.11)\n",
            "Requirement already satisfied: lxml>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from python-docx) (4.2.6)\n",
            "According to the American Kennel Club (AKC), a Bulldog's disposition should be \"equable and kind, resolute, and courageous (not vicious or aggressive), and demeanor should be pacifist and dignified. These attributes should be countenanced by the expression and behavior\".\n",
            "Breeders have worked to reduce/remove aggression from these dogs. Most have a friendly, patient, but stubborn nature. Bulldogs are recognized as excellent family pets because of their tendency to form strong bonds with children.\n",
            "Generally, Bulldogs are known for getting along well with children, other dogs, and other pets.\n",
            "##THIS IS SECOND TIME ['According to the American Kennel Club (AKC), a Bulldog\\'s disposition should be \"equable and kind, resolute, and courageous (not vicious or aggressive), and demeanor should be pacifist and dignified. These attributes should be countenanced by the expression and behavior\".', 'Breeders have worked to reduce/remove aggression from these dogs. Most have a friendly, patient, but stubborn nature. Bulldogs are recognized as excellent family pets because of their tendency to form strong bonds with children.', 'Generally, Bulldogs are known for getting along well with children, other dogs, and other pets.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pfuSbrEhIOV2"
      },
      "source": [
        "###**SENT2VEC**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "beIPCGFcKZ7d",
        "outputId": "182ea73e-db20-40f3-d793-0526eceee1a5"
      },
      "source": [
        "!pip install git+https://github.com/epfml/sent2vec\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/epfml/sent2vec\n",
            "  Cloning https://github.com/epfml/sent2vec to /tmp/pip-req-build-jrrcxxfe\n",
            "  Running command git clone -q https://github.com/epfml/sent2vec /tmp/pip-req-build-jrrcxxfe\n",
            "Building wheels for collected packages: sent2vec\n",
            "  Building wheel for sent2vec (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sent2vec: filename=sent2vec-0.0.0-cp37-cp37m-linux_x86_64.whl size=1130632 sha256=1670193526f2691c3824679355bd17c26dd6bba4311ed5fd69d3af15d92a9117\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-1ab46ndz/wheels/f5/1a/52/b5f36e8120688b3f026ac0cefe9c6544905753c51d8190ff17\n",
            "Successfully built sent2vec\n",
            "Installing collected packages: sent2vec\n",
            "Successfully installed sent2vec-0.0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5x_Rty5DomAi",
        "outputId": "1ced8016-a549-4f34-98b7-9f7a89a917d8"
      },
      "source": [
        "!pip install cython\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: cython in /usr/local/lib/python3.7/dist-packages (0.29.23)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LA9a3Cx_IR8h"
      },
      "source": [
        "import sent2vec\n",
        "model = sent2vec.Sent2vecModel()\n",
        "model.load_model('model.bin')\n",
        "emb = model.embed_sentence(\"once upon a time .\") \n",
        "embs = model.embed_sentences([\"first sentence .\", \"another sentence\"])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0N3v1OeM0qwy"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ayd1sSr-0s8H"
      },
      "source": [
        "###Word2Vec"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dNKijwPa0wRN"
      },
      "source": [
        "#Install tabula\n",
        "!pip install tabula-py\n",
        "\n",
        "# Import the required Module\n",
        "#import tabula\n",
        "from tabula.io import read_pdf,convert_into\n",
        "# Read a PDF File\n",
        "df = read_pdf(\"/content/drive/MyDrive/GoogleCollabWork/wild-sands.pdf\", pages='all')[0]\n",
        "print(df)\n",
        "# convert PDF into CSV\n",
        "convert_into(\"/content/drive/MyDrive/GoogleCollabWork/wild-sands.pdf\", \"wildsand.csv\", output_format=\"csv\", pages='all')\n",
        "print(df)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q9HhqKdjISyQ"
      },
      "source": [
        "!pip install spacy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "id": "8NT8Alyf8STJ",
        "outputId": "915f0a8e-0c82-46fa-8dbc-dfdee1a51a1d"
      },
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import gensim\n",
        "import spacy\n",
        "import en_core_web_sm\n",
        "#nlp = en_core_web_sm.load()\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "data=pd.read_csv('/content/drive/MyDrive/GoogleCollabWork/wild-sands.csv')\n",
        "\n",
        "\n",
        "data.head()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>By Chris Mason</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Copyright 2012 by Christopher A. Mason</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>This ebook is an authorized free edition from ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Although you do not have to pay for this book,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>fully protected by international Copyright law...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text\n",
              "0                                     By Chris Mason\n",
              "1             Copyright 2012 by Christopher A. Mason\n",
              "2  This ebook is an authorized free edition from ...\n",
              "3  Although you do not have to pay for this book,...\n",
              "4  fully protected by international Copyright law..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 110
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "id": "1eVPG1lpii3u",
        "outputId": "2df901ef-d41c-4128-f92c-01726a9813f7"
      },
      "source": [
        "s = pd.Series([i for i in range(4082)])\n",
        "data=data.set_index([s, s])\n",
        "data=data.rename_axis(\"Id\",axis='columns')\n",
        "data.head()  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Id</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <th>0</th>\n",
              "      <td>By Chris Mason</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <th>1</th>\n",
              "      <td>Copyright 2012 by Christopher A. Mason</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <th>2</th>\n",
              "      <td>This ebook is an authorized free edition from ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <th>3</th>\n",
              "      <td>Although you do not have to pay for this book,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <th>4</th>\n",
              "      <td>fully protected by international Copyright law...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "Id                                                text\n",
              "0 0                                     By Chris Mason\n",
              "1 1             Copyright 2012 by Christopher A. Mason\n",
              "2 2  This ebook is an authorized free edition from ...\n",
              "3 3  Although you do not have to pay for this book,...\n",
              "4 4  fully protected by international Copyright law..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 128
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xYvre8NZJ9UL",
        "outputId": "fe86f33f-87cc-423e-dc0b-ab7c70461f25"
      },
      "source": [
        "ref_sent=data.loc[446]['text']\n",
        "print(ref_sent)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "446    When he could see the water, there were often ...\n",
            "Name: text, dtype: object\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TFR6eR0FJM16"
      },
      "source": [
        "ref_sent_vec=nlp('ref_sent')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LzdRTGPGtuZ2"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DdXrPIS8TSeE"
      },
      "source": [
        "all_docs=[nlp(row) for row in data['text']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Tlp70CMaOKb"
      },
      "source": [
        "sims=[]\n",
        "doc_id=[]\n",
        "for i in range(len(all_docs)):\n",
        "  sim=all_docs[i].similarity(ref_sent_vec)\n",
        "  sims.append(sim)\n",
        "  doc_id.append(i)\n",
        "  sims_docs=pd.DataFrame(list(zip(doc_id,sims)),columns=['doc_id','sims'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rsfJHSnxcUJ9"
      },
      "source": [
        "sims_docs_sorted=sims_docs.sort_values(by=\"sims\",ascending=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mfp3cosDcsTG"
      },
      "source": [
        "top5_sims_docs=data.iloc[sims_docs_sorted['doc_id'][1:6]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "59ZUTdV21FAK"
      },
      "source": [
        "#sims_docs_sorted=sims_docs_sorted['sims'][1:6].to_frame()\n",
        "#sims_docs_sorted=sims_docs_sorted.to_frame()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D1nWi1uteue6",
        "outputId": "6771714b-0df4-459c-d685-95aee46f3c10"
      },
      "source": [
        "print(data.loc[446]['text'].values)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['When he could see the water, there were often huge pillars or pyramids of stone standing out']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zItxGT5m48gh"
      },
      "source": [
        "print(sims_docs_sorted)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4z02YKoO5QlI",
        "outputId": "36a331f4-1cb0-486d-e893-1815c9de4d7d"
      },
      "source": [
        "print(top5_sims_docs)\n",
        "top5_sims_docs[\"Id\"]=[13,4040,3130,3363,483]\n",
        "print(top5_sims_docs)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Id                                                      text    Id\n",
            "13   13    \f                                      Chapter...    13\n",
            "4040 4040  \f                                       Chapte...  4040\n",
            "3130 3130                                            Robin?”  3130\n",
            "3363 3363                                            Robin?”  3363\n",
            "483  483                                        little pond.   483\n",
            "Id                                                      text    Id\n",
            "13   13    \f                                      Chapter...    13\n",
            "4040 4040  \f                                       Chapte...  4040\n",
            "3130 3130                                            Robin?”  3130\n",
            "3363 3363                                            Robin?”  3363\n",
            "483  483                                        little pond.   483\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PUlWgnBV5pzn"
      },
      "source": [
        "hello=pd.DataFrame(sims_docs_sorted['sims'][1:6])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F8PikiaG_lZC",
        "outputId": "d606ff04-f800-48e7-9bbb-add3320973a0"
      },
      "source": [
        "print(hello)\n",
        "hello['Id']=[13,4040,3130,3363,483]\n",
        "print(hello)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "          sims    Id\n",
            "13    0.462434    13\n",
            "4040  0.459097  4040\n",
            "3130  0.454773  3130\n",
            "3363  0.454773  3363\n",
            "483   0.450695   483\n",
            "          sims    Id\n",
            "13    0.462434    13\n",
            "4040  0.459097  4040\n",
            "3130  0.454773  3130\n",
            "3363  0.454773  3363\n",
            "483   0.450695   483\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5i9m0b-PEykI",
        "outputId": "41cb50ed-8970-4a95-af3d-47d40cb177e7"
      },
      "source": [
        "photo_df = top5_sims_docs.set_index('Id', drop=False)\n",
        "print(photo_df)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Id                                                 text    Id\n",
            "Id                                                           \n",
            "13    \f                                      Chapter...    13\n",
            "4040  \f                                       Chapte...  4040\n",
            "3130                                            Robin?”  3130\n",
            "3363                                            Robin?”  3363\n",
            "483                                        little pond.   483\n",
            "Id                                                 text    Id\n",
            "Id                                                           \n",
            "13    \f                                      Chapter...    13\n",
            "4040  \f                                       Chapte...  4040\n",
            "3130                                            Robin?”  3130\n",
            "3363                                            Robin?”  3363\n",
            "483                                        little pond.   483\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yxJMZmrhsABn"
      },
      "source": [
        "top_sim_scores=pd.merge(hello,top5_sims_docs,on=\"Id\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tGDh9YZX_NdM",
        "outputId": "403673c1-a84e-4045-c643-bfd388f4d61e"
      },
      "source": [
        "print(top_sim_scores)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "       sims    Id                                               text\n",
            "0  0.462434    13  \f                                      Chapter...\n",
            "1  0.459097  4040  \f                                       Chapte...\n",
            "2  0.454773  3130                                            Robin?”\n",
            "3  0.454773  3363                                            Robin?”\n",
            "4  0.450695   483                                       little pond.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KJ9dicKTL4ep",
        "outputId": "da851cbc-c068-48ce-8d56-3c4d2b5afbf2"
      },
      "source": [
        "for (text,sim) in zip(top_sim_scores['text'],top_sim_scores['sims']):\n",
        "  print(\"The Top 5 similiar sentences are: {}\\n with similiarity score as {:.2f}\\n\".format(text,sim))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The Top 5 similiar sentences are: \f                                      Chapter 1: Prelude\n",
            " with similiarity score as 0.46\n",
            "\n",
            "The Top 5 similiar sentences are: \f                                       Chapter 13: Coda\n",
            " with similiarity score as 0.46\n",
            "\n",
            "The Top 5 similiar sentences are: Robin?”\n",
            " with similiarity score as 0.45\n",
            "\n",
            "The Top 5 similiar sentences are: Robin?”\n",
            " with similiarity score as 0.45\n",
            "\n",
            "The Top 5 similiar sentences are: little pond.\n",
            " with similiarity score as 0.45\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p0tnokAmOk0D"
      },
      "source": [
        "### Embedding Using Seq2Learner Website"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r6pkG5t_OkfZ"
      },
      "source": [
        "!pip install seqlearner\n",
        "!pip install keras\n",
        "!pip install tensorflow\n",
        "!pip install pomegranate\n",
        "!pip install matplotlib\n",
        "!pip install numpy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        },
        "id": "UyNbBYu1PAvA",
        "outputId": "d102fedb-d669-48cd-f909-7d5d27275e85"
      },
      "source": [
        "import pandas as pd\n",
        "from seqlearner import Freq2Vec\n",
        "sequences = pd.read_csv(\"/content/drive/MyDrive/GoogleCollabWork/harry-potter-the-chamber-secrets.csv\", header=None)\n",
        "freq2vec = Freq2Vec(sequences, word_length=3, window_size=5, emb_dim=25, loss=\"mean_squared_error\", epochs=250)\n",
        "freq2vec.freq2vec_maker()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-312-960a47ab0624>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mseqlearner\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFreq2Vec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0msequences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/drive/MyDrive/GoogleCollabWork/harry-potter-the-chamber-secrets.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mfreq2vec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFreq2Vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0memb_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m25\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"mean_squared_error\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m250\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mfreq2vec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfreq2vec_maker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'module' object is not callable"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lVtoS9iROuqm"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fZqlxmrjOvS2"
      },
      "source": [
        "### Sent2Vec Using Seq2Learner Website"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7wne3CS1cbKl",
        "outputId": "749ed4af-180b-4a43-d3f2-5a671cb664ce"
      },
      "source": [
        "#!pip install WordEmbedder\n",
        "!pip install git+https://github.com/EliHei/SeqLearn"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/EliHei/SeqLearn\n",
            "  Cloning https://github.com/EliHei/SeqLearn to /tmp/pip-req-build-y9xx1q4u\n",
            "  Running command git clone -q https://github.com/EliHei/SeqLearn /tmp/pip-req-build-y9xx1q4u\n",
            "Requirement already satisfied (use --upgrade to upgrade): seqlearner==0.0.7 from git+https://github.com/EliHei/SeqLearn in /usr/local/lib/python3.7/dist-packages\n",
            "Building wheels for collected packages: seqlearner\n",
            "  Building wheel for seqlearner (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for seqlearner: filename=seqlearner-0.0.7-cp37-none-any.whl size=55079 sha256=7666f8c004875514a69850a855a171530a5f62d1fc897ca0d8c90d13ca22a5f5\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-zhyfg8id/wheels/e4/04/aa/60285a30369d74b5c0b3b0ecbb261627877be5bafe633f64d8\n",
            "Successfully built seqlearner\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 246
        },
        "id": "CEobz_5LO1U-",
        "outputId": "3169c892-a33c-43c6-bc4f-9a2147a45715"
      },
      "source": [
        "import pandas as pd\n",
        "import seqlearner\n",
        "#from seqlearner import Sent2Vec\n",
        "sequences = pd.read_csv(\"/content/drive/MyDrive/GoogleCollabWork/harry-potter-the-chamber-secrets.csv\", header=None)\n",
        "sent2vec = seqlearner.Sent2Vec(sequences, word_length=3, emb_dim=25, epoch=100, lr=0.2, wordNgrams=5, loss=\"hs\", neg=20, thread=10, t=0.0000005, dropoutK=2, bucket=4000000)\n",
        "\n",
        "encoding = sent2vec.sent2vec_maker()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-320-e0e9c01baaa1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#from seqlearner import Sent2Vec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0msequences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/drive/MyDrive/GoogleCollabWork/harry-potter-the-chamber-secrets.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0msent2vec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mseqlearner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSent2Vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0memb_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m25\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwordNgrams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"hs\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthread\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0000005\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropoutK\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbucket\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4000000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mencoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msent2vec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msent2vec_maker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'module' object is not callable"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pr6OSdOrVMWW"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wn-bDJ7XVNW5"
      },
      "source": [
        "###ANOTHER WEBSITE METHODS FOR SENT2VEC"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6363l0j4VSMa",
        "outputId": "920555cc-2231-4ae2-c738-cafa43daaafb"
      },
      "source": [
        "!pip install git+https://github.com/facebookresearch/fastText.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/facebookresearch/fastText.git\n",
            "  Cloning https://github.com/facebookresearch/fastText.git to /tmp/pip-req-build-w4m7frum\n",
            "  Running command git clone -q https://github.com/facebookresearch/fastText.git /tmp/pip-req-build-w4m7frum\n",
            "Requirement already satisfied (use --upgrade to upgrade): fasttext==0.9.2 from git+https://github.com/facebookresearch/fastText.git in /usr/local/lib/python3.7/dist-packages\n",
            "Requirement already satisfied: pybind11>=2.2 in /usr/local/lib/python3.7/dist-packages (from fasttext==0.9.2) (2.6.2)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from fasttext==0.9.2) (57.0.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from fasttext==0.9.2) (1.20.3)\n",
            "Building wheels for collected packages: fasttext\n",
            "  Building wheel for fasttext (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fasttext: filename=fasttext-0.9.2-cp37-cp37m-linux_x86_64.whl size=3089820 sha256=3680a59922033999b9c71abb62432248260156b905002c2f1b5ad04dad14f610\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-xc2be8g1/wheels/69/f8/19/7f0ab407c078795bc9f86e1f6381349254f86fd7d229902355\n",
            "Successfully built fasttext\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "boKBcnUbVxEn"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KaqJVKHewiwY"
      },
      "source": [
        "### Model from Website https://radimrehurek.com/gensim/models/word2vec.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fZEtO5pNwvWh"
      },
      "source": [
        "from gensim.test.utils import common_texts\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "model = Word2Vec(sentences=common_texts, window=5, min_count=1, workers=4)\n",
        "model.save(\"word2vec.model\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tafg9SjRw8NJ",
        "outputId": "5f846606-bb0a-420c-9329-6b961b9dad95"
      },
      "source": [
        "model = Word2Vec.load(\"word2vec.model\")\n",
        "model.train([[\"hello\", \"world\"]], total_examples=1, epochs=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 323
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m61gzcmRxCfG"
      },
      "source": [
        "vector = model.wv['computer']  # get numpy vector of a word\n",
        "sims = model.wv.most_similar('computer', topn=10)  # get other similar words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SEuNMoqOxHq_"
      },
      "source": [
        "from gensim.models import KeyedVectors\n",
        "\n",
        "# Store just the words + their trained embeddings.\n",
        "word_vectors = model.wv\n",
        "word_vectors.save(\"word2vec.wordvectors\")\n",
        "\n",
        "# Load back with memory-mapping = read-only, shared across processes.\n",
        "wv = KeyedVectors.load(\"word2vec.wordvectors\", mmap='r')\n",
        "\n",
        "vector = wv['computer']  # Get numpy vector of a word"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2VLujFYixSWI"
      },
      "source": [
        "from gensim.test.utils import datapath\n",
        "\n",
        "# Load a word2vec model stored in the C *text* format.\n",
        "wv_from_text = KeyedVectors.load_word2vec_format(datapath('word2vec_pre_kv_c'), binary=False)\n",
        "# Load a word2vec model stored in the C *binary* format.\n",
        "wv_from_bin = KeyedVectors.load_word2vec_format(datapath(\"euclidean_vectors.bin\"), binary=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wL2JR4EFxXRl"
      },
      "source": [
        "word_vectors = model.wv\n",
        "del model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A1MtnGQkxjdD",
        "outputId": "5d15ab48-72b3-469e-adfb-0a1071046c5b"
      },
      "source": [
        "from gensim.models import Phrases\n",
        "\n",
        "# Train a bigram detector.\n",
        "bigram_transformer = Phrases(common_texts)\n",
        "\n",
        "# Apply the trained MWE detector to a corpus, using the result to train a Word2vec model.\n",
        "model = Word2Vec(bigram_transformer[common_texts], min_count=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
            "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iaKGMQd1x5bM"
      },
      "source": [
        "import gensim.downloader\n",
        "# Show all available models in gensim-data\n",
        "print(list(gensim.downloader.info()['models'].keys()))\n",
        "['fasttext-wiki-news-subwords-300',\n",
        " 'conceptnet-numberbatch-17-06-300',\n",
        " 'word2vec-ruscorpora-300',\n",
        " 'word2vec-google-news-300',\n",
        " 'glove-wiki-gigaword-50',\n",
        " 'glove-wiki-gigaword-100',\n",
        " 'glove-wiki-gigaword-200',\n",
        " 'glove-wiki-gigaword-300',\n",
        " 'glove-twitter-25',\n",
        " 'glove-twitter-50',\n",
        " 'glove-twitter-100',\n",
        " 'glove-twitter-200',\n",
        " '__testing_word2vec-matrix-synopsis']\n",
        "\n",
        "# Download the \"glove-twitter-25\" embeddings\n",
        "glove_vectors = gensim.downloader.load('glove-twitter-25')\n",
        "\n",
        "# Use the downloaded vectors as usual:\n",
        "glove_vectors.most_similar('twitter')\n",
        "[('facebook', 0.948005199432373),\n",
        " ('tweet', 0.9403423070907593),\n",
        " ('fb', 0.9342358708381653),\n",
        " ('instagram', 0.9104824066162109),\n",
        " ('chat', 0.8964964747428894),\n",
        " ('hashtag', 0.8885937333106995),\n",
        " ('tweets', 0.8878158330917358),\n",
        " ('tl', 0.8778461217880249),\n",
        " ('link', 0.8778210878372192),\n",
        " ('internet', 0.8753897547721863)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qnOgrVa9yAT5"
      },
      "source": [
        "from gensim.test.utils import datapath\n",
        "from gensim.models.word2vec import LineSentence\n",
        "sentences = LineSentence(datapath('lee_background.cor'))\n",
        "for sentence in sentences:\n",
        "    pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9x-0QHMnyOg8"
      },
      "source": [
        "from gensim.models import Word2Vec\n",
        "sentences = [[\"cat\", \"say\", \"meow\"], [\"dog\", \"say\", \"woof\"]]\n",
        "model = Word2Vec(sentences, min_count=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HePkD01TyP_X",
        "outputId": "c6e39dcb-c775-4731-ea00-31a44fd56738"
      },
      "source": [
        "from gensim.models import Word2Vec\n",
        "sentences = [[\"cat\", \"say\", \"meow\"], [\"dog\", \"say\", \"woof\"]]\n",
        "model = Word2Vec(min_count=1)\n",
        "model.build_vocab(sentences)  # prepare the model vocabulary\n",
        "model.train(sentences, total_examples=model.corpus_count, epochs=model.epochs)  # train word vectors"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 30)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 334
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wAWXE1NlzCO_"
      },
      "source": [
        "### Doc2Vec"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HR3asELtzFGi"
      },
      "source": [
        "from gensim.test.utils import common_texts\n",
        "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
        "documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(common_texts)]\n",
        "model = Doc2Vec(documents, vector_size=5, window=2, min_count=1, workers=4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dfD-fLz8zInD"
      },
      "source": [
        "from gensim.test.utils import get_tmpfile\n",
        "\n",
        "fname = get_tmpfile(\"my_doc2vec_model\")\n",
        "\n",
        "model.save(fname)\n",
        "model = Doc2Vec.load(fname)  # you can continue training with the loaded model!"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z3Ptf0HLzNf0",
        "outputId": "d1d6c5ea-7cc5-4e6b-d142-e8f2e35be074"
      },
      "source": [
        "vector = model.infer_vector([\"system\", \"response\"])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 0.02928366  0.14237206 -0.02267986  0.08773571 -0.00779648  0.08778913\n",
            "  0.14808035  0.11708865  0.03329771 -0.16866688 -0.06759381  0.07259941\n",
            " -0.135719    0.06634886  0.01646323  0.10016165 -0.03790294  0.0559139\n",
            " -0.00920132  0.19620982  0.15503085 -0.08860038 -0.07688845  0.16444044\n",
            "  0.24925159 -0.02824616 -0.12013091  0.02220447 -0.05598956  0.08269601\n",
            "  0.00604811  0.05485802  0.07325888  0.12196562 -0.11325276  0.08562492\n",
            "  0.08239219  0.04570205 -0.15454614  0.01678174  0.01239824 -0.07281502\n",
            "  0.08888572 -0.13090032 -0.1082538  -0.07066141  0.00935184  0.12933943\n",
            " -0.02474796 -0.02513623]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 162
        },
        "id": "UdFfk5YTzlvG",
        "outputId": "e73fa4c0-026a-4786-e0b7-846e4b566b18"
      },
      "source": [
        "model.dv['doc003']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-345-c4c7bc1579e7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'doc003'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: 'Doc2Vec' object has no attribute 'dv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1TM3vdD3yYAJ"
      },
      "source": [
        "from gensim.test.utils import datapath\n",
        "from gensim.models.doc2vec import TaggedLineDocument\n",
        "for document in TaggedLineDocument(datapath(\"head500.noblanks.cor\")):\n",
        "    pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AkqL_et80Ip1",
        "outputId": "6f1ee4fa-4ba6-47de-d908-bb0da0da72b5"
      },
      "source": [
        "print(document)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TaggedDocument(['function', 'group', 'alcohol', 'molecul', 'carbon', 'atom', 'attach', 'carbon', 'atom', 'hydrogen', 'atom', 'chemistri', 'alcohol', 'organ', 'compound', 'hydroxyl', 'group', 'bound', 'carbon', 'atom', 'alkyl', 'substitut', 'alkyl', 'group', 'gener', 'formula', 'simpl', 'acycl', 'alcohol', 'layman', 'term', 'word', 'alcohol', 'arab', 'al-', 'refer', 'ethanol', 'grain', 'alcohol', 'older', 'spirit', 'wine', 'alcohol', 'beverag', 'ethanol', 'colorless', 'volatil', 'liquid', 'mild', 'odor', 'obtain', 'ferment', 'sugar', 'industri', 'commonli', 'obtain', 'ethylen', 'hydrat', 'reaction', 'ethylen', 'water', 'presenc', 'phosphor', 'acid', 'ethanol', 'ed', 'encyclopedia', 'chemic', 'technolog', 'ed', 'york', 'john', 'wilei', 'son', 'ethanol', 'wide', 'depress', 'world', 'thousand', 'year', 'sens', 'underli', 'term', 'alcohol', 'addict', 'alcohol', 'alcohol', 'clarifi', 'adject', 'isopropyl', 'alcohol', 'propan--ol', 'wood', 'alcohol', 'methyl', 'alcohol', 'methanol', 'suffix', 'appear', 'offici', 'iupac', 'chemic', 'alcohol', 'major', 'subset', 'alcohol', 'primari', 'secondari', 'tertiari', 'base', 'number', 'carbon', 'atom', 'group', 'carbon', 'shown', 'red', 'bond', 'ethanol', 'simpl', 'alcohol', 'simplest', 'secondari', 'alcohol', 'isopropyl', 'alcohol', 'propan--ol', 'simpl', 'tertiari', 'alcohol', 'tert-butyl', 'alcohol', 'phenol', 'parent', 'compound', 'phenol', 'hydroxyl', 'group', 'attach', 'benzen', 'ring', 'alcohol', 'differ', 'suffici', 'properti', 'warrant', 'separ', 'treatment', 'carbohydr', 'sugar', 'sugar', 'alcohol', 'import', 'class', 'compound', 'multipl', 'alcohol', 'function', 'group', 'sucros', 'common', 'sugar', 'hydroxyl', 'group', 'molecul', 'sorbitol', 'attribut', 'polyol', 'nomenclatur', 'occurr', 'toxic', 'suffici', 'simpl', 'aliphat', 'alcohol', 'requir', 'separ', 'treatment', 'simpl', 'alcohol', 'simplest', 'commonli', 'alcohol', 'methanol', 'ethanol', 'methanol', 'obtain', 'distil', 'wood', 'call', 'wood', 'alcohol', 'cheap', 'commod', 'chemic', 'product', 'carbon', 'monoxid', 'react', 'hydrogen', 'high', 'pressur', 'methanol', 'intox', 'poison', 'toxic', 'breakdown', 'toxic', 'enzym', 'alcohol', 'dehydrogenas', 'liver', 'form', 'formic', 'acid', 'formaldehyd', 'perman', 'blind', 'destruct', 'optic', 'nerv', 'scientist', 'chemistri', 'archiv', 'retriev', 'familiar', 'role', 'alcohol', 'beverag', 'ethanol', 'highli', 'control', 'industri', 'solvent', 'raw', 'materi', 'avoid', 'high', 'tax', 'ethanol', 'consumpt', 'addit', 'ad', 'unpalat', 'denatonium', 'benzoat', 'poison', 'methanol', 'ethanol', 'form', 'gener', 'denatur', 'alcohol', 'methanol', 'refer', 'methyl', 'spirit', 'meth', 'surgic', 'spirit', 'alcohol', 'widespread', 'methanol', 'ethanol', 'propanol', 'butanol', 'ethanol', 'produc', 'ferment', 'process', 'ferment', 'agent', 'bacterium', 'clostridium', 'acetobutylicum', 'feed', 'cellulos', 'sugar', 'saccharomyc', 'yeast', 'produc', 'ethanol', 'nomenclatur', 'systemat', 'name', 'iupac', 'system', 'alkan', 'chain', 'lose', 'termin', 'add', 'ol', 'methanol', 'ethanol', 'william', 'reusch', 'organ', 'chemistri', 'retriev', 'septemb', 'posit', 'hydroxyl', 'group', 'number', 'alkan', 'ol', 'propan--ol', 'ch', 'ch', 'ch', 'propan--ol', 'ch', 'ch', 'ch', 'posit', 'number', 'written', 'iupac', 'higher', 'prioriti', 'group', 'present', 'aldehyd', 'keton', 'carboxyl', 'acid', 'prefix', 'hydroxi', 'ch', 'coch', 'exampl', 'simpl', 'alcohol', 'exampl', 'alcohol', 'name', 'common', 'name', 'alcohol', 'take', 'alkyl', 'group', 'add', 'word', 'alcohol', 'methyl', 'alcohol', 'ethyl', 'alcohol', 'tert-butyl', 'alcohol', 'propyl', 'alcohol', 'alcohol', 'isopropyl', 'alcohol', 'depend', 'hydroxyl', 'group', 'bond', 'st', 'carbon', 'propan', 'chain', 'isopropyl', 'alcohol', 'occasion', 'call', 'alcohol', 'mention', 'alcohol', 'classifi', 'primari', 'secondari', 'tertiari', 'common', 'name', 'alkyl', 'group', 'prefix', 'ch', 'coh', 'tertiari', 'alcohol', 'commonli', 'tert-butyl', 'alcohol', 'name', 'iupac', 'rule', 'indic', 'propan', 'chain', 'methyl', 'hydroxyl', 'group', 'attach', 'middl', 'carbon', 'primari', 'alcohol', 'gener', 'formula', 'rch', 'secondari', 'alcohol', 'gener', 'formula', 'tertiari', 'alcohol', 'gener', 'formula', 'hydrogen', 'bond', 'strength', 'order', 'boil', 'point', 'order', 'acid', 'order', 'etymolog', 'word', 'alcohol', 'appear', 'english', 'centuri', 'loan', 'french', 'medic', 'latin', 'ultim', 'arab', 'al-', 'al', 'arab', 'definit', 'articl', 'english', 'fine', 'powder', 'produc', 'sublim', 'natur', 'miner', 'stibnit', 'form', 'antimoni', 'sulfid', 'sb', 'essenc', 'spirit', 'substanc', 'antisept', 'eyelin', 'introduct', 'word', 'european', 'terminolog', 'alchemi', 'date', 'centuri', 'latin', 'translat', 'work', 'rhaze', 'art', 'distil', 'bartholomew', 'translat', 'john', 'vigo', 'introduc', 'word', 'term', 'barbar', 'moorish', 'author', 'fine', 'powder', 'william', 'johnson', 'lexicon', 'gloss', 'word', 'sive', 'extens', 'word', 'refer', 'fluid', 'obtain', 'distil', 'includ', 'alcohol', 'wine', 'distil', 'essenc', 'wine', 'libaviu', 'vini', 'alcohol', 'vel', 'vinum', 'johnson', 'gloss', 'alcohol', 'vini', 'quando', 'omni', 'vini', 'vino', 'ita', 'ut', 'donec', 'totum', 'aut', 'fundo', 'word', 'mean', 'restrict', 'spirit', 'wine', 'ethanol', 'centuri', 'extend', 'famili', 'substanc', 'call', 'modern', 'chemistri', 'current', 'arab', 'alcohol', 're-introduc', 'western', 'usag', 'classic', 'arab', 'word', 'sura', 'liter', 'spirit', 'word', 'origin', 'english', 'word', 'ghoul', 'star', 'algol', 'physic', 'chemic', 'properti', 'hydroxyl', 'group', 'gener', 'alcohol', 'molecul', 'polar', 'group', 'form', 'hydrogen', 'bond', 'compound', 'hydrogen', 'bond', 'mean', 'alcohol', 'protic', 'solvent', 'oppos', 'solubl', 'trend', 'alcohol', 'tendenc', 'polar', 'promot', 'solubl', 'water', 'carbon', 'chain', 'resist', 'methanol', 'ethanol', 'propanol', 'miscibl', 'water', 'hydroxyl', 'group', 'win', 'short', 'carbon', 'chain', 'butanol', 'four-carbon', 'chain', 'moder', 'solubl', 'balanc', 'trend', 'alcohol', 'carbon', 'higher', 'effect', 'insolubl', 'water', 'hydrocarbon', 'chain', 'domin', 'simpl', 'alcohol', 'miscibl', 'organ', 'solvent', 'hydrogen', 'bond', 'alcohol', 'tend', 'higher', 'boil', 'point', 'compar', 'hydrocarbon', 'ether', 'boil', 'point', 'alcohol', 'ethanol', 'compar', 'hydrocarbon', 'hexan', 'common', 'constitu', 'gasolin', 'diethyl', 'ether', 'alcohol', 'water', 'show', 'acid', 'basic', 'properti', 'o-h', 'group', 'pk', 'gener', 'slightli', 'weaker', 'acid', 'water', 'react', 'strong', 'base', 'sodium', 'hydrid', 'reactiv', 'metal', 'sodium', 'salt', 'result', 'call', 'alkoxid', 'gener', 'formula', 'ro', 'oxygen', 'atom', 'lone', 'pair', 'nonbond', 'electron', 'render', 'weakli', 'basic', 'presenc', 'strong', 'acid', 'sulfur', 'acid', 'methanol', 'acid', 'basic', 'methanol', 'alcohol', 'undergo', 'oxid', 'give', 'aldehyd', 'keton', 'carboxyl', 'acid', 'dehydr', 'alken', 'react', 'form', 'ester', 'compound', 'activ', 'undergo', 'nucleophil', 'substitut', 'reaction', 'lone', 'pair', 'electron', 'oxygen', 'hydroxyl', 'group', 'alcohol', 'nucleophil', 'detail', 'reaction', 'alcohol', 'section', 'applic', 'alcohol', 'beverag', 'ethanol', 'fuel', 'scientif', 'medic', 'industri', 'util', 'ethanol', 'form', 'alcohol', 'beverag', 'consum', 'human', 'pre-histor', 'time', 'solut', 'ethylen', 'glycol', 'water', 'commonli', 'antifreez', 'alcohol', 'ethanol', 'methanol', 'alcohol', 'fuel', 'fuel', 'perform', 'increas', 'forc', 'induct', 'intern', 'combust', 'engin', 'inject', 'alcohol', 'air', 'intak', 'turbocharg', 'supercharg', 'pressur', 'air', 'cool', 'pressur', 'air', 'provid', 'denser', 'air', 'charg', 'fuel', 'power', 'alcohol', 'applic', 'industri', 'scienc', 'reagent', 'solvent', 'toxic', 'abil', 'dissolv', 'non-polar', 'substanc', 'ethanol', 'solvent', 'medic', 'drug', 'perfum', 'veget', 'essenc', 'vanilla', 'organ', 'synthesi', 'alcohol', 'serv', 'versatil', 'intermedi', 'ethanol', 'antisept', 'disinfect', 'skin', 'inject', 'iodin', 'soap', 'common', 'restaur', 'conveni', 'requir', 'dry', 'due', 'volatil', 'compound', 'alcohol', 'preserv', 'specimen', 'product', 'industri', 'alcohol', 'produc', 'wai', 'ferment', 'glucos', 'produc', 'sugar', 'hydrolysi', 'starch', 'presenc', 'yeast', 'temperatur', 'produc', 'ethanol', 'instanc', 'convers', 'invertas', 'glucos', 'fructos', 'convers', 'glucos', 'ethanol', 'direct', 'hydrat', 'ethylen', 'ethylen', 'hydrat', 'alken', 'crack', 'fraction', 'distil', 'crude', 'oil', 'catalyst', 'phosphor', 'acid', 'high', 'temperatur', 'pressur', 'methanol', 'produc', 'synthesi', 'ga', 'carbon', 'monoxid', 'equival', 'hydrogen', 'ga', 'combin', 'produc', 'methanol', 'copper', 'zinc', 'oxid', 'aluminum', 'oxid', 'catalyst', 'pressur', 'atm', 'endogen', 'inevit', 'human', 'amount', 'alcohol', 'bodi', 'time', 'drink', 'alcohol', 'beverag', 'live', 'process', 'call', 'endogen', 'ethanol', 'product', 'bacteria', 'intestin', 'alcohol', 'ferment', 'form', 'respir', 'metabol', 'method', 'produc', 'alcohol', 'wast', 'product', 'metabol', 'result', 'format', 'carbon', 'dioxid', 'water', 'human', 'bodi', 'quantiti', 'alcohol', 'produc', 'benign', 'bacteria', 'laboratori', 'synthesi', 'method', 'exist', 'prepar', 'alcohol', 'laboratori', 'primari', 'alkyl', 'halid', 'react', 'aqueou', 'naoh', 'koh', 'primari', 'alcohol', 'nucleophil', 'aliphat', 'substitut', 'secondari', 'tertiari', 'alkyl', 'halid', 'give', 'elimin', 'alken', 'product', 'aldehyd', 'keton', 'reduc', 'sodium', 'borohydrid', 'lithium', 'aluminium', 'hydrid', 'acid', 'workup', 'reduct', 'reduct', 'alken', 'engag', 'acid', 'catalys', 'hydrat', 'reaction', 'concentr', 'sulfur', 'acid', 'catalyst', 'secondari', 'tertiari', 'alcohol', 'alken', 'reliabl', 'organ', 'synthesi', 'alken', 'react', 'nb', 'water', 'format', 'reaction', 'grignard', 'reagent', 'react', 'carbonyl', 'group', 'secondari', 'tertiari', 'alcohol', 'noyori', 'asymmetr', 'hydrogen', 'asymmetr', 'reduct', 'amin', 'convert', 'diazonium', 'salt', 'hydrolyz', 'format', 'secondari', 'alcohol', 'reduct', 'hydrat', 'shown', 'reaction', 'deproton', 'alcohol', 'behav', 'weak', 'acid', 'undergo', 'deproton', 'deproton', 'reaction', 'produc', 'alkoxid', 'salt', 'perform', 'strong', 'base', 'sodium', 'hydrid', 'n-butyllithium', 'sodium', 'potassium', 'metal', 'water', 'similar', 'pk', 'alcohol', 'sodium', 'hydroxid', 'equilibrium', 'set', 'li', 'left', 'note', 'base', 'deproton', 'alcohol', 'strong', 'base', 'alkoxid', 'creat', 'highli', 'moistur', 'sensit', 'chemic', 'reagent', 'acid', 'alcohol', 'affect', 'stabil', 'alkoxid', 'ion', 'electron-withdraw', 'group', 'attach', 'carbon', 'hydroxyl', 'group', 'serv', 'stabil', 'alkoxid', 'form', 'result', 'greater', 'acid', 'hand', 'presenc', 'electron-don', 'group', 'result', 'stabl', 'alkoxid', 'ion', 'form', 'result', 'scenario', 'unstabl', 'alkoxid', 'ion', 'form', 'tend', 'accept', 'proton', 'reform', 'origin', 'alcohol', 'alkyl', 'halid', 'alkoxid', 'give', 'rise', 'ether', 'williamson', 'ether', 'synthesi', 'nucleophil', 'substitut', 'group', 'good', 'leav', 'group', 'nucleophil', 'substitut', 'reaction', 'neutral', 'alcohol', 'react', 'reaction', 'oxygen', 'proton', 'give', 'leav', 'group', 'water', 'stabl', 'nucleophil', 'substitut', 'place', 'instanc', 'tertiari', 'alcohol', 'react', 'hydrochlor', 'acid', 'produc', 'tertiari', 'alkyl', 'halid', 'hydroxyl', 'group', 'replac', 'chlorin', 'atom', 'unimolecular', 'nucleophil', 'substitut', 'primari', 'secondari', 'alcohol', 'react', 'hydrochlor', 'acid', 'activ', 'zinc', 'chlorid', 'need', 'altern', 'convers', 'perform', 'thionyl', 'chlorid', 'simpl', 'convers', 'alcohol', 'alkyl', 'chlorid', 'alcohol', 'convert', 'alkyl', 'bromid', 'hydrobrom', 'acid', 'phosphoru', 'tribromid', 'deoxygen', 'alcohol', 'deoxygen', 'alkan', 'tributyltin', 'hydrid', 'complex', 'radic', 'substitut', 'reaction', 'dehydr', 'alcohol', 'nucleophil', 'react', 'roh', 'produc', 'ether', 'water', 'dehydr', 'reaction', 'reaction', 'rare', 'manufactur', 'diethyl', 'ether', 'elimin', 'reaction', 'alcohol', 'produc', 'alken', 'reaction', 'gener', 'obei', 'zaitsev', 'rule', 'state', 'stabl', 'substitut', 'alken', 'form', 'tertiari', 'alcohol', 'elimin', 'easili', 'room', 'temperatur', 'primari', 'alcohol', 'requir', 'higher', 'temperatur', 'diagram', 'acid', 'catalys', 'dehydr', 'ethanol', 'produc', 'ethen', 'control', 'elimin', 'reaction', 'elimin', 'carbon', 'disulfid', 'iodomethan', 'esterif', 'form', 'ester', 'alcohol', 'carboxyl', 'acid', 'reaction', 'fischer', 'esterif', 'perform', 'reflux', 'catalyst', 'concentr', 'sulfur', 'acid', 'order', 'drive', 'equilibrium', 'produc', 'good', 'yield', 'ester', 'water', 'remov', 'excess', 'apparatu', 'ester', 'prepar', 'reaction', 'alcohol', 'acid', 'chlorid', 'presenc', 'base', 'pyridin', 'type', 'ester', 'prepar', 'tosyl', 'tosyl', 'ester', 'reaction', 'alcohol', 'chlorid', 'pyridin', 'oxid', 'primari', 'alcohol', 'oxid', 'aldehyd', 'carboxyl', 'acid', 'r-co', 'oxid', 'secondari', 'alcohol', 'termin', 'keton', 'rc', 'stage', 'tertiari', 'alcohol', 'resist', 'oxid', 'direct', 'oxid', 'primari', 'alcohol', 'carboxyl', 'acid', 'proce', 'aldehyd', 'transform', 'aldehyd', 'hydrat', 'reaction', 'water', 'oxid', 'carboxyl', 'acid', 'mechan', 'oxid', 'primari', 'alcohol', 'carboxyl', 'acid', 'aldehyd', 'aldehyd', 'hydrat', 'interrupt', 'oxid', 'primari', 'alcohol', 'aldehyd', 'level', 'perform', 'reaction', 'absenc', 'water', 'aldehyd', 'hydrat', 'form', 'reagent', 'transform', 'primari', 'alcohol', 'aldehyd', 'suitabl', 'oxid', 'secondari', 'alcohol', 'keton', 'includ', 'reagent', 'collin', 'reagent', 'cro', 'pdc', 'pcc', 'activ', 'dmso', 'result', 'reaction', 'dmso', 'electrophil', 'chlorid', 'swern', 'oxid', 'oxid', 'complex', 'oxid', 'hyperval', 'iodin', 'compound', 'dess-martin', 'periodinan', 'acid', 'catalyt', 'presenc', 'excess', 'nmo', 'lei', 'oxid', 'catalyt', 'tempo', 'presenc', 'excess', 'bleach', 'naocl', 'oxid', 'oxid', 'alcohol', 'aldehyd', 'keton', 'allyl', 'benzyl', 'alcohol', 'oxid', 'presenc', 'alcohol', 'select', 'oxid', 'manganes', 'dioxid', 'mno', 'reagent', 'oxid', 'secondari', 'alcohol', 'keton', 'ineffici', 'oxid', 'primari', 'alcohol', 'aldehyd', 'includ', 'chromium', 'trioxid', 'cro', 'mixtur', 'sulfur', 'acid', 'aceton', 'jone', 'oxid', 'keton', 'cyclohexanon', 'presenc', 'aluminium', 'isopropoxid', 'oxid', 'direct', 'oxid', 'primari', 'alcohol', 'carboxyl', 'acid', 'carri', 'potassium', 'permangan', 'kmno', 'jone', 'oxid', 'pdc', 'dmf', 'heyn', 'oxid', 'ruthenium', 'tetroxid', 'ruo', 'tempo', 'oxid', 'primari', 'alcohol', 'carboxyl', 'acid', 'alcohol', 'possess', 'hydroxi', 'group', 'locat', 'adjac', 'carbon', 'suffer', 'oxid', 'breakag', 'carbon-carbon', 'bond', 'oxid', 'sodium', 'period', 'naio', 'lead', 'tetraacet', 'pb', 'oac', 'result', 'gener', 'carbonyl', 'group', 'oxid', 'breakag', 'carbon-carbon', 'bond', 'toxic', 'alcohol', 'odor', 'nasal', 'passag', 'ethanol', 'form', 'alcohol', 'beverag', 'consum', 'human', 'pre-histor', 'time', 'varieti', 'hygien', 'dietari', 'medicin', 'religi', 'recreat', 'reason', 'consumpt', 'larg', 'dose', 'result', 'drunken', 'intox', 'lead', 'hangov', 'effect', 'wear', 'depend', 'dose', 'regular', 'acut', 'respiratori', 'failur', 'death', 'chronic', 'medic', 'repercuss', 'alcohol', 'impair', 'judgment', 'catalyst', 'reckless', 'irrespons', 'behavior', 'ld', 'ethanol', 'rat', 'mg', 'kg', 'robert', 'gabl', 'reprint', 'addict', 'ratio', 'correspond', 'kg', 'lb', 'man', 'drink', 'shot', 'proof', 'alcohol', 'litr', 'bottl', 'beer', 'abv', 'ld', 'translat', 'human', 'gener', 'agre', 'fatal', 'dose', 'human', 'rat', 'drink', 'consum', 'sit', 'kg', 'lb', 'man', 'alcohol', 'substanti', 'poison', 'ethanol', 'partli', 'longer', 'metabol', 'metabol', 'produc', 'toxic', 'substanc', 'methanol', 'wood', 'alcohol', 'instanc', 'oxid', 'alcohol', 'dehydrogenas', 'enzym', 'liver', 'poison', 'formaldehyd', 'blind', 'death', 'effect', 'treatment', 'prevent', 'formaldehyd', 'toxic', 'methanol', 'ingest', 'administ', 'ethanol', 'alcohol', 'dehydrogenas', 'higher', 'affin', 'ethanol', 'prevent', 'methanol', 'bind', 'act', 'substrat', 'remain', 'methanol', 'time', 'excret', 'kidnei', 'remain', 'formaldehyd', 'convert', 'formic', 'acid', 'excret'], [249])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PKSfOlgk0s8a"
      },
      "source": [
        "###  Document similarity queries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y3gsUQYB0vH8",
        "outputId": "50daa278-af70-454e-f660-c8a6b76d075c"
      },
      "source": [
        "from gensim.test.utils import common_corpus, common_dictionary, get_tmpfile\n",
        "from gensim.similarities import Similarity\n",
        "\n",
        "index_tmpfile = get_tmpfile(\"index\")\n",
        "query = [(1, 2), (6, 1), (7, 2)]\n",
        "\n",
        "index = Similarity(index_tmpfile, common_corpus, num_features=len(common_dictionary))  # build the index\n",
        "similarities = index[query]  # get similarities between the query and all index documents\n",
        "print(\"Similarity score is\",similarities)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Similarity score is [0.38490018 0.4082483  0.33333334 0.27216554 0.57735026 0.\n",
            " 0.         0.         0.        ]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gensim/similarities/docsim.py:518: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
            "  result = numpy.hstack(shard_results)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eG-8ZcfD1zB6",
        "outputId": "bfe7f408-8044-44dc-ecda-bb0f67ce274c"
      },
      "source": [
        "from gensim.test.utils import common_corpus, common_dictionary, get_tmpfile\n",
        "\n",
        "index_tmpfile = get_tmpfile(\"index\")\n",
        "batch_of_documents = common_corpus[:]  # only as example\n",
        "index = Similarity(index_tmpfile, common_corpus, num_features=len(common_dictionary))  # build the index\n",
        "# the batch is simply an iterable of documents, aka gensim corpus:\n",
        "for similarities in index[batch_of_documents]:\n",
        "    print(similarities)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.99999994 0.23570226 0.28867513 0.23570226 0.         0.\n",
            " 0.         0.         0.        ]\n",
            "[0.23570226 1.         0.4082483  0.33333334 0.70710677 0.\n",
            " 0.         0.         0.23570226]\n",
            "[0.28867513 0.4082483  1.         0.61237246 0.28867513 0.\n",
            " 0.         0.         0.        ]\n",
            "[0.23570226 0.33333334 0.61237246 1.         0.         0.\n",
            " 0.         0.         0.        ]\n",
            "[0.         0.70710677 0.28867513 0.         0.99999994 0.\n",
            " 0.         0.         0.        ]\n",
            "[0.         0.         0.         0.         0.         1.\n",
            " 0.70710677 0.57735026 0.        ]\n",
            "[0.         0.         0.         0.         0.         0.70710677\n",
            " 0.99999994 0.81649655 0.40824828]\n",
            "[0.         0.         0.         0.         0.         0.57735026\n",
            " 0.81649655 0.99999994 0.6666666 ]\n",
            "[0.         0.23570226 0.         0.         0.         0.\n",
            " 0.40824828 0.6666666  0.99999994]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gensim/similarities/docsim.py:518: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
            "  result = numpy.hstack(shard_results)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qe3yIGcb19_O",
        "outputId": "ed664b9b-f1bf-4ea5-bda8-0d470ecc479f"
      },
      "source": [
        "from gensim.test.utils import common_corpus, common_dictionary, get_tmpfile\n",
        "\n",
        "index_tmpfile = get_tmpfile(\"index\")\n",
        "index = Similarity(index_tmpfile, common_corpus, num_features=len(common_dictionary))  # build the index\n",
        "\n",
        "for similarities in index:  # yield similarities of the 1st indexed document, then 2nd...\n",
        "    pass"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gensim/similarities/docsim.py:518: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
            "  result = numpy.hstack(shard_results)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ClzBR1W_2DFe"
      },
      "source": [
        "from gensim.test.utils import common_corpus, common_dictionary\n",
        "from gensim.similarities import MatrixSimilarity\n",
        "\n",
        "query = [(1, 2), (5, 4)]\n",
        "index = MatrixSimilarity(common_corpus, num_features=len(common_dictionary))\n",
        "sims = index[query]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9HyYvpI32KJz",
        "outputId": "b316ec5d-feaf-4240-a5a9-41b1d343c50d"
      },
      "source": [
        "from gensim.corpora.textcorpus import TextCorpus\n",
        "from gensim.test.utils import datapath, get_tmpfile\n",
        "from gensim.similarities import Similarity\n",
        "\n",
        "corpus = TextCorpus(datapath('testcorpus.mm'))\n",
        "index_temp = get_tmpfile(\"index\")\n",
        "index = Similarity(index_temp, corpus, num_features=400)  # create index\n",
        "\n",
        "query = next(iter(corpus))\n",
        "result = index[query]  # search similar to `query` in index\n",
        "\n",
        "for sims in index[corpus]:  # if you have more query documents, you can submit them all at once, in a batch\n",
        "    pass\n",
        "\n",
        "# There is also a special syntax for when you need similarity of documents in the index\n",
        "# to the index itself (i.e. queries=indexed documents themselves). This special syntax\n",
        "# uses the faster, batch queries internally and **is ideal for all-vs-all pairwise similarities**:\n",
        "for similarities in index:  # yield similarities of the 1st indexed document, then 2nd...\n",
        "    pass"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gensim/similarities/docsim.py:518: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
            "  result = numpy.hstack(shard_results)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bTnajgcN2TRj"
      },
      "source": [
        "from gensim.corpora.textcorpus import TextCorpus\n",
        "from gensim.test.utils import datapath, get_tmpfile\n",
        "from gensim.similarities import Similarity\n",
        "\n",
        "temp_fname = get_tmpfile(\"index\")\n",
        "output_fname = get_tmpfile(\"saved_index\")\n",
        "\n",
        "corpus = TextCorpus(datapath('testcorpus.txt'))\n",
        "index = Similarity(output_fname, corpus, num_features=400)\n",
        "\n",
        "index.save(output_fname)\n",
        "loaded_index = index.load(output_fname)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cUE-GdDV0PEG",
        "outputId": "e048167e-4f0e-4820-886d-0b512c854cdd"
      },
      "source": [
        "from gensim.corpora.textcorpus import TextCorpus\n",
        "from gensim.test.utils import datapath\n",
        "from gensim.similarities import Similarity\n",
        "\n",
        "corpus = TextCorpus(datapath('testcorpus.txt'))\n",
        "index = Similarity('temp', corpus, num_features=400)\n",
        "similarities = index.similarity_by_id(1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gensim/similarities/docsim.py:518: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
            "  result = numpy.hstack(shard_results)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "teEyFuI_2cj_"
      },
      "source": [
        "from gensim.test.utils import common_texts\n",
        "from gensim.corpora import Dictionary\n",
        "from gensim.models import Word2Vec\n",
        "from gensim.similarities import SoftCosineSimilarity,SparseTermSimilarityMatrix\n",
        "#from gensim.similarities import SparseTermSimilarityMatrix\n",
        "from gensim.similarities import WordEmbeddingSimilarityIndex\n",
        "\n",
        "model = Word2Vec(common_texts, vector_size=20, min_count=1)  # train word-vectors\n",
        "termsim_index = WordEmbeddingSimilarityIndex(model.wv)\n",
        "dictionary = Dictionary(common_texts)\n",
        "bow_corpus = [dictionary.doc2bow(document) for document in common_texts]\n",
        "similarity_matrix = SparseTermSimilarityMatrix(termsim_index, dictionary)  # construct similarity matrix\n",
        "docsim_index = SoftCosineSimilarity(bow_corpus, similarity_matrix, num_best=10)\n",
        "\n",
        "query = 'graph trees computer'.split()  # make a query\n",
        "sims = docsim_index[dictionary.doc2bow(query)]  # calculate similarity of query to each doc from bow_corpus\n",
        "print(sims)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "glC4cy6d4VWy"
      },
      "source": [
        "### Tf-IDF MODELLING"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mmiGxWib4az1",
        "outputId": "64953ba3-a4ee-4671-ef97-2e14caa34b07"
      },
      "source": [
        "import gensim.downloader as api\n",
        "from gensim.models import TfidfModel\n",
        "from gensim.corpora import Dictionary\n",
        "\n",
        "dataset = api.load(\"text8\")\n",
        "dct = Dictionary(dataset)  # fit dictionary\n",
        "corpus = [dct.doc2bow(line) for line in dataset]  # convert corpus to BoW format\n",
        "\n",
        "model = TfidfModel(corpus)  # fit model\n",
        "vector = model[corpus[0]]  # apply model to the first corpus document"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[==================================================] 100.0% 31.6/31.6MB downloaded\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FrBa1EuO4kc6",
        "outputId": "cb06fab1-5f2b-4e50-c821-39a3a70be931"
      },
      "source": [
        "print(vector)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[(1, 0.006704047545684609), (2, 0.0030255603220721273), (3, 0.003156168449586299), (4, 0.0036673470201144674), (5, 0.004575122435127926), (6, 0.0028052608258295926), (7, 0.004064820137019515), (8, 0.00014963587508918375), (9, 0.0007492665180478759), (10, 0.004142807322609117), (11, 0.004149816941645728), (12, 0.0077498817493309525), (13, 0.00656024165742503), (14, 0.003891486499758776), (15, 0.005476877392392166), (16, 0.0018233938817994433), (17, 0.0032209070754237084), (18, 0.0017737283389229173), (19, 0.0023373507198140124), (20, 0.003725514968930464), (21, 0.00590342512385848), (22, 0.003072401062545206), (23, 0.0006668171096292247), (24, 0.0017594266221832493), (25, 0.004202080158963513), (26, 0.002967397324595724), (27, 0.004709756138185673), (28, 0.0014819657487289912), (29, 0.0031562459553171694), (30, 0.0031999829254611097), (31, 0.001215574949729317), (32, 0.003843126241898761), (33, 0.006499414537896336), (34, 0.004546489373863172), (35, 0.003855345600331923), (36, 0.004631358254286941), (37, 0.0033393576216394117), (38, 0.008862366517905295), (39, 0.005471809309284401), (40, 0.002420389691699831), (41, 0.0032450256679229764), (42, 0.0037466176880179758), (43, 0.005046439550933583), (44, 0.004575122435127926), (45, 0.0017993832406368628), (46, 0.0020574652728579546), (47, 0.009237638705224305), (48, 0.004459969548562313), (49, 0.018035534860263287), (50, 0.00903732020117277), (51, 0.008978913462318317), (52, 0.0023304759543738285), (53, 0.003682641329586604), (54, 0.004612379138391923), (55, 0.004556833486121144), (56, 0.0011939038031690697), (57, 2.6657621716188957e-05), (58, 0.0021615630847404772), (59, 0.0019039524360541717), (60, 0.0023941045596380275), (61, 0.008291338219166304), (62, 0.0032914545563888053), (63, 0.0077498817493309525), (64, 0.005291651332753796), (65, 0.008288039973755134), (66, 0.0020784214405569585), (67, 0.0021915903407992232), (68, 0.006359839657649205), (69, 0.016149368891044475), (70, 0.003120742674469207), (71, 0.009041815014928456), (72, 0.0032260698476640026), (73, 0.006143425087649577), (74, 0.010178233569371548), (75, 0.004730009057296232), (76, 0.005097590884003234), (77, 2.4851152829512927e-05), (78, 0.004026069503754151), (79, 0.00436905456109441), (80, 0.0016516920661684236), (81, 0.0012235475822093677), (82, 0.0024470951644187354), (83, 0.000513154038941833), (84, 0.002364414933857586), (85, 0.0005448633286620751), (86, 0.007314422153534317), (87, 0.0006618243983774284), (88, 8.522903221806888e-05), (89, 0.007474791370974179), (90, 0.002233878505007749), (91, 0.0010862875602653704), (92, 0.003529288231481582), (93, 0.0005022187295527164), (94, 0.0030792129017414354), (95, 0.0008746326541962667), (96, 0.004319764827769512), (97, 0.0036439372739091986), (98, 0.0007197428502658651), (99, 0.005205409585118467), (100, 0.0001065708197676177), (101, 0.005726810819983659), (102, 0.010178233569371548), (103, 0.0009158808294883222), (104, 0.0019183966563681338), (105, 3.904032576162575e-05), (106, 0.0064289571673278355), (107, 0.021414323874678808), (108, 0.09160410212434394), (109, 0.5427960527876164), (110, 0.3071551737479831), (111, 0.010178233569371548), (112, 0.024259695486236584), (113, 0.3956111284690035), (114, 0.19060282761110503), (115, 0.011224067773017892), (116, 0.07503634364088867), (117, 0.003163447972901711), (118, 0.0007064920768726969), (120, 0.008795715952977295), (121, 0.004173611015886935), (122, 0.0036047449452107903), (123, 0.011392826705006954), (124, 0.005658213342038267), (125, 0.005480499963475241), (126, 0.0013990173621148312), (127, 0.008288039973755134), (128, 0.00032509485652374055), (129, 0.0021727533942733942), (130, 0.0031706845326691306), (131, 0.004949062551139832), (132, 0.015792293472954355), (133, 0.009566459778266864), (134, 0.013204510649302276), (135, 0.005447337753413767), (136, 0.00015232071438021105), (137, 0.0048799442152677956), (138, 0.0032940022667188222), (139, 0.0018655592554384973), (140, 0.0021727533942733942), (141, 0.0027330174047964497), (142, 0.0017578040822413888), (143, 0.00656024165742503), (144, 0.001002984513633349), (145, 0.0018204266835586188), (146, 0.0030124400670575903), (147, 0.0027168065002897857), (148, 0.005707989755621536), (149, 0.0012745360368386117), (150, 0.0053215299292903575), (151, 0.005835028880753795), (152, 0.008086565162078861), (153, 0.00011985332655665936), (154, 0.0005659094750754488), (155, 0.002954771143640896), (156, 0.008726295979231696), (157, 0.00453198147856653), (158, 0.0028692280854457864), (159, 0.002905297209097752), (160, 0.003086055633738092), (161, 0.005447337753413767), (162, 0.006902582651824663), (163, 0.005021499812552176), (164, 0.0033152159895020543), (165, 0.0015700066072129155), (166, 0.0003083036077492477), (167, 0.006414794576773642), (168, 0.0010600483713598464), (169, 0.0011609968709728844), (170, 0.0016844560308771513), (172, 0.009566459778266864), (173, 0.007474791370974179), (174, 0.04851939097247317), (175, 0.011169195550715529), (176, 0.006567233027455108), (177, 0.003501937274338205), (178, 0.002967397324595724), (179, 0.003979202586837869), (180, 0.005549094494509803), (181, 0.00437072427177618), (182, 0.0032993335289622804), (184, 0.0021578513489960583), (185, 0.013011866280105444), (186, 0.0007150165053053836), (187, 0.0026365317359547612), (188, 0.0010663748321193022), (189, 0.002435144100725132), (190, 0.0036944041797413517), (191, 0.0037047033311246877), (192, 0.008795715952977295), (193, 0.006308187143608161), (194, 0.0005985268243689562), (195, 0.004719711699848922), (196, 0.0039161379593174945), (197, 0.06817091950792369), (198, 0.01312048331485006), (199, 0.007351930818993706), (200, 0.0017737283389229173), (201, 0.37297919366665255), (202, 0.29571070025416574), (203, 0.019132919556533727), (204, 0.011224067773017892), (205, 0.008520625574620522), (206, 0.012235475822093675), (207, 0.008520625574620522), (208, 0.011224067773017892), (209, 0.004556833486121144), (210, 0.0013119040040790013), (211, 0.0064289571673278355), (212, 0.002359855849924461), (213, 0.0031562459553171694), (214, 0.0017719023399977302), (215, 0.00590342512385848), (216, 0.00024192816829530063), (217, 0.009132399365725203), (218, 0.008288039973755134), (219, 0.00038954742251225545), (220, 0.001605303977016286), (221, 0.09560227751430855), (222, 0.005735605681201885), (223, 0.0035760046511191606), (224, 0.0024470951644187354), (225, 0.0025254331757030594), (226, 0.003174835346343899), (227, 0.003510999282474801), (228, 0.008770383650571511), (229, 0.003134843896913292), (230, 0.0009306011396053992), (231, 0.0005294635336269773), (232, 0.004013231528242067), (233, 0.0077498817493309525), (234, 9.762952897308651e-05), (235, 0.0003773972899254821), (236, 0.00018821715199780523), (237, 0.00040038987497627835), (238, 0.0019209594722839202), (239, 6.218278693860421e-05), (240, 0.0001654590196263831), (241, 0.00022227236987640823), (242, 0.0014422269111857814), (243, 0.0037466176880179758), (244, 0.014587151054395547), (245, 0.013616290715134267), (246, 0.02168644380017574), (247, 0.0008868641694614587), (248, 8.728759380202386e-05), (249, 0.005328090373645632), (250, 0.005796656104091995), (251, 0.003966048173113494), (252, 0.007739663683795608), (253, 0.0027359717951318916), (254, 0.0029236603544517833), (255, 0.0031706845326691306), (256, 0.0029422497454785128), (257, 0.0010203233593024685), (258, 0.01105252456094967), (259, 0.013372254409958777), (260, 0.015499763498661905), (261, 0.0022222273288165376), (262, 0.004275695725644014), (263, 0.0008589252637620388), (264, 0.001329155439958817), (265, 1.5994573029713374e-05), (266, 0.013562879769696456), (267, 0.001940702741053815), (268, 0.00210055166256963), (269, 0.004145669109583152), (270, 0.005291651332753796), (271, 0.0012116200906947597), (272, 0.00278827580797511), (273, 0.003106772020774026), (274, 0.003045575385394732), (275, 0.003653896516795931), (276, 0.006143425087649577), (277, 0.009132399365725203), (278, 0.005735605681201885), (279, 0.003624213858162274), (280, 0.004156842881113917), (281, 0.0022580337790522776), (282, 0.020390363536012937), (283, 0.028291066710191333), (284, 0.013408095091369218), (285, 0.004612379138391923), (286, 0.003510999282474801), (287, 0.009132399365725203), (288, 0.009566459778266864), (289, 0.0012709280658418278), (290, 0.0031795771162102084), (291, 0.004321653181573843), (292, 0.0022814043181450783), (293, 0.00021440073995388317), (294, 0.009805267325348007), (295, 0.005948467866320346), (296, 0.006863017579869494), (297, 0.010178233569371548), (298, 0.004156842881113917), (299, 0.003492929368065079), (300, 0.001544842338977366), (301, 0.0013856253823110868), (302, 0.002203006434246346), (303, 0.0018353213733140515), (304, 0.0033232203574480672), (305, 0.0006224557459454057), (306, 0.004593635794013783), (307, 0.003260343767199692), (308, 0.0008568290482275019), (309, 0.004949062551139832), (310, 0.0051238318900972005), (311, 0.0009306011396053992), (312, 7.55076698540194e-05), (314, 0.00011746677365012682), (315, 0.0019342958532138244), (316, 0.015212151722142747), (317, 0.0007018857821295965), (318, 0.0062838884771550035), (319, 0.0007981011911829036), (320, 0.0009688854393261934), (321, 0.0038341100116214533), (322, 0.0007207263316812906), (323, 0.0016390946358651687), (324, 0.0009029155955177102), (325, 0.054071174178161846), (326, 0.019341367146485496), (327, 0.017451550128669453), (328, 0.008086565162078861), (329, 0.0016338465616377782), (330, 0.007908851783515837), (331, 0.0021878040908344452), (332, 0.0011725293079006996), (333, 0.0016443610280152995), (334, 0.0021248503290227875), (335, 0.00557655161595022), (336, 0.0002949118362635956), (337, 0.0006909879535193662), (338, 0.0053215299292903575), (339, 0.004996965616390439), (340, 0.0015167527609571913), (341, 0.0007510626491478926), (342, 0.0013203924642690918), (343, 0.0033152159895020543), (344, 0.0022377823100812114), (345, 0.0043372887600351484), (346, 0.0004173004760718374), (347, 0.0010212810508337146), (348, 0.008795715952977295), (349, 0.002978887516717126), (350, 0.001989641727951152), (351, 0.0042908598715693195), (352, 0.047832298891334314), (353, 0.002047095336373661), (354, 0.00047572908925796293), (355, 0.0007657257365544594), (356, 0.0007613118285235538), (357, 0.010141417266689774), (358, 0.0006825978309589405), (359, 0.011780969341820213), (360, 0.001999622931842174), (361, 0.0033634992537767703), (362, 0.004709756138185673), (363, 0.002009670603729324), (364, 0.004321653181573843), (365, 0.0048799442152677956), (366, 0.0073540213472545034), (367, 0.0010064371920717892), (368, 0.01917629706767594), (369, 0.0025301704474728175), (370, 0.009856943277006798), (371, 0.006308187143608161), (372, 0.0011250483660610253), (373, 0.00413028893711584), (374, 0.0035855240506405977), (375, 0.020185758203734333), (376, 0.006244261784749965), (377, 0.0077498817493309525), (378, 0.0017794872001858687), (379, 0.0016578317216706774), (380, 0.00218025997602155), (381, 0.00407359236998065), (382, 0.008520625574620522), (383, 0.00022948422296110532), (384, 0.007024794507843711), (385, 0.009878866299788765), (386, 0.002505222952880402), (387, 0.002203006434246346), (388, 0.0028864714040054036), (389, 0.008101947599403206), (390, 0.0016926044925497396), (391, 0.0014491640260229988), (392, 0.0024391225319386842), (393, 0.0022574542943411026), (394, 0.00195684000524516), (395, 0.010829777529481508), (396, 0.0070407309584325185), (397, 0.0008725624527217905), (398, 0.006863017579869494), (399, 0.0071381079582262695), (400, 0.0014986379605661424), (401, 0.027397198097175612), (402, 0.008288039973755134), (403, 0.0035760046511191606), (404, 0.021414323874678808), (405, 0.0005530574397950452), (406, 0.009566459778266864), (407, 0.1192887580446873), (408, 0.002636706123362083), (409, 0.002664045186822816), (410, 0.005994896754786175), (411, 0.010410819170236934), (412, 0.005775843260942577), (413, 0.006143425087649577), (414, 0.003757280769989727), (415, 0.004131889837384435), (416, 0.00279391626246612), (417, 0.0031419442385775018), (418, 0.0071381079582262695), (419, 0.009132399365725203), (420, 0.006704047545684609), (421, 0.007908851783515837), (422, 0.0051238318900972005), (423, 0.0034235809416766345), (424, 0.006042799820036782), (425, 0.0027712507646221736), (426, 0.0018513567904171713), (427, 0.004118235266191549), (428, 0.006042799820036782), (429, 0.0014631346797181807), (430, 0.003222336220516769), (431, 0.0006964442902160434), (432, 0.0011344014403713164), (433, 0.0044346657598288985), (434, 0.010247663780194401), (435, 0.005262352939961816), (436, 0.009644544796796661), (437, 0.004750537534771031), (438, 0.01293140733922393), (439, 0.02998140674002475), (440, 0.014281550010642238), (441, 0.009132399365725203), (442, 0.02947339882919912), (443, 0.008288039973755134), (444, 0.008301357010039208), (445, 0.0024791126362027883), (446, 0.0071381079582262695), (447, 0.0005958913305649103), (448, 0.004902633662674003), (449, 0.00378972975866274), (450, 0.0028992254913701165), (451, 0.010178233569371548), (452, 0.007548368870848529), (453, 0.0026572856200201833), (454, 0.0008289158608353387), (455, 0.0020574652728579546), (456, 0.010178233569371548), (457, 0.005620956638774268), (458, 0.0009061464743094097), (459, 0.003177955967558057), (460, 0.004771349172576808), (461, 0.006869269158170225), (462, 0.0019342958532138244), (463, 0.00380532650374769), (464, 0.0038007030420786746), (465, 0.0023388119966785653), (466, 0.0035951038911987247), (467, 0.0034015943989598283), (468, 0.005084214436203641), (469, 0.010178233569371548), (470, 0.006308187143608161), (471, 0.017041251149241043), (472, 0.007133089869491159), (473, 0.005514407453778687), (474, 0.008367240892101717), (475, 0.0026796807652841215), (476, 0.005775843260942577), (477, 0.0020197856348135577), (478, 0.001508248310376597), (479, 0.002501967386721097), (480, 0.0042908598715693195), (481, 0.0033888315561825554), (482, 0.0027770577650778716), (483, 0.002415984375229623), (484, 0.002285433211911962), (485, 0.0017312235614315285), (486, 0.002993460795173276), (487, 0.0022145095635533493), (488, 0.0007543841638940695), (489, 0.004013231528242067), (490, 0.0068562996357358855), (491, 0.0049256695272139085), (492, 0.0030521481434346463), (493, 0.0015622281095032447), (494, 0.004775615212676279), (495, 0.0013679523273211003), (496, 0.003106772020774026), (497, 0.002670726318710158), (498, 0.002096110034931158), (499, 0.009566459778266864), (500, 0.0021652839739731877), (501, 0.0012660523597007565), (502, 0.0028811555769603947), (503, 0.0013992960139786253), (504, 0.001343985010354193), (505, 0.0027801523629306896), (506, 0.004949062551139832), (507, 0.003397237259366789), (508, 0.004593635794013783), (509, 0.009715181840424273), (510, 0.007242205770108791), (511, 0.004245817129107454), (512, 0.004417993547991748), (513, 0.00656024165742503), (514, 0.002081942608594248), (515, 0.005021499812552176), (516, 0.0012804909370527178), (517, 0.000196055241734018), (518, 0.0010520561498620997), (519, 0.003397237259366789), (520, 0.0005151453995223673), (521, 0.005714788287694291), (522, 0.0015034354839060385), (523, 0.0073540213472545034), (524, 0.0026462455567296842), (525, 0.0007063663169369755), (526, 0.0023721923599111824), (527, 0.010178233569371548), (528, 0.0070407309584325185), (529, 0.002006313939788303), (530, 0.009076680966216381), (531, 0.0012619523033212782), (532, 0.006092273754579926), (533, 0.01031994866775662), (534, 0.006241485348938414), (535, 0.01198979350957235), (536, 0.004347492078068996), (537, 0.011224067773017892), (538, 0.013759931557008828), (539, 0.003214848265598887), (540, 0.007242205770108791), (541, 0.0037360294353079855), (542, 0.005291651332753796), (543, 0.006367364132936699), (544, 0.019132919556533727), (545, 0.0064289571673278355), (546, 0.0039756656089058335), (547, 0.009132399365725203), (548, 0.005224737224923979), (549, 0.0023050820410268876), (550, 0.007623513433131588), (551, 0.000751541406688131), (552, 0.008026463056484134), (553, 0.005994896754786175), (554, 0.010178233569371548), (555, 0.00047070522227174314), (556, 0.0032520170379530546), (557, 0.0017007971994799141), (558, 0.001966605863411247), (559, 0.006143425087649577), (560, 0.0018145097355082747), (561, 0.003032163482804515), (562, 0.0022444411631614013), (563, 0.0005997857085548875), (564, 0.0021730658403751857), (565, 0.001205691517962174), (566, 0.0013504838853802624), (567, 0.003163447972901711), (568, 0.0014216049069904087), (569, 0.0005972085020315317), (570, 0.008288039973755134), (571, 0.005262352939961816), (572, 0.002706105605654185), (573, 0.006493171957060111), (574, 0.0012888040149372387), (575, 0.010042999625104352), (576, 0.00335809613749283), (577, 0.010092879101867167), (578, 0.001335363159355079), (579, 0.0016496458664102627), (580, 0.007735138963547792), (581, 0.002594025728447754), (582, 0.002695480070056606), (583, 0.0023296624617994578), (584, 0.0038682734292970994), (585, 0.0028663761014880587), (586, 0.0018115599402008028), (587, 0.005050866351406119), (588, 0.0034839749214911155), (589, 0.006932447790692348), (590, 0.0176060141990697), (591, 0.0073540213472545034), (592, 0.001983024086556747), (593, 0.0022145095635533493), (594, 0.0015672324916007035), (595, 0.006614507722322579), (596, 0.00234719435056701), (597, 0.0037466176880179758), (598, 0.004575122435127926), (599, 0.0009858386493612316), (600, 0.0035233799391534044), (601, 0.003257124328269543), (602, 0.0013267902964100677), (603, 0.007268086676005538), (604, 0.0020130347518770757), (605, 0.004159575381472124), (606, 0.005041421462198474), (607, 0.003252665278265571), (608, 0.0036946878112068377), (609, 0.0015522688666165122), (610, 0.0013396679263812578), (611, 0.0016364683169789198), (612, 0.0029737502595359147), (613, 0.004264814720971226), (614, 0.0008944524201235499), (615, 0.004839104771496004), (616, 0.0024018963259333963), (617, 0.013725367305383778), (618, 0.0022935234179930685), (619, 0.0074932353760359515), (620, 0.0020231724371765516), (621, 0.012827087176932043), (622, 0.007908851783515837), (623, 0.0161552214310098), (624, 0.019085396690307233), (625, 0.00040911978984734936), (626, 0.011224067773017892), (627, 0.003910389096079479), (628, 0.0016015594999051134), (629, 0.003086055633738092), (630, 0.006385209637467346), (631, 0.0030629029462178377), (632, 0.009542008520110406), (633, 0.008951129240324845), (634, 0.014484411540217582), (635, 0.011224067773017892), (636, 0.0023371364050386767), (637, 0.0027995778819391286), (638, 0.0052336126458701875), (639, 0.0007150165053053836), (640, 0.007514561539979454), (641, 0.0051238318900972005), (642, 0.0025609818741054355), (643, 0.004593635794013783), (644, 0.004670048061808618), (645, 0.005658213342038267), (646, 0.01937916913198603), (647, 0.025098668354484186), (648, 0.004321653181573843), (649, 0.0030998349059368087), (650, 0.0018716802367305548), (651, 0.005291651332753796), (652, 0.0014261632784913739), (653, 0.0073540213472545034), (654, 0.0026153552305347807), (655, 0.005817183376223151), (656, 0.001999622931842174), (657, 0.0052336126458701875), (658, 0.0043680546367026556), (659, 0.005896994911040377), (660, 0.0017514578346060755), (661, 0.0024232401813895194), (662, 0.0013989070690915166), (663, 0.0021504551586013668), (664, 0.003510999282474801), (665, 0.001116502632610584), (666, 0.0051777240410307785), (667, 0.0031244562190064894), (668, 0.005658213342038267), (669, 0.0031562459553171694), (670, 0.01031994866775662), (671, 0.002183528452478312), (672, 0.003510999282474801), (673, 0.005281569857076367), (674, 0.0048799442152677956), (675, 0.002043654468704102), (676, 0.030981857832312238), (677, 0.0025066314393351603), (678, 0.0004339035821999788), (679, 9.371672189069931e-05), (680, 0.0015747703248597206), (681, 0.003529288231481582), (682, 0.005817183376223151), (683, 0.011224067773017892), (684, 0.005948467866320346), (685, 0.00022923680338450222), (686, 0.00019416271132492792), (687, 0.0005919467013398399), (688, 0.0013701501715832796), (689, 0.0006068003492854081), (690, 0.0023429973525362123), (691, 0.0032120771067579807), (692, 0.009993931232780877), (693, 0.0021248503290227875), (694, 0.00458302454861152), (695, 0.0007909847577005248), (696, 0.010178233569371548), (697, 0.004670048061808618), (698, 0.016056036523477136), (699, 0.010178233569371548), (700, 0.00391506228909213), (701, 0.007616964817514637), (702, 0.0045032602908634594), (703, 0.004719711699848922), (704, 0.002802521921544784), (705, 0.0022614196575149463), (706, 0.0010844664136119367), (707, 0.00200296472682775), (708, 0.00290665631797858), (709, 0.004506998650631875), (710, 0.0006157706671351623), (711, 0.0011157288810941344), (712, 0.0013900761814653448), (713, 0.005620956638774268), (714, 0.008937146500264687), (715, 0.028162923833730074), (716, 0.017041251149241043), (717, 0.008795715952977295), (719, 0.000244012173901743), (720, 0.0027168065002897857), (721, 0.0011978228178897569), (722, 0.0016602714020078416), (723, 0.003966048173113494), (724, 0.003058868955523419), (725, 0.0008258460330842118), (726, 0.003268061532462881), (727, 0.0037047033311246877), (728, 0.00281669133367391), (729, 0.0017881669162748788), (730, 0.007242205770108791), (731, 0.0037360294353079855), (732, 0.005948467866320346), (733, 0.009993931232780877), (734, 0.023061895691959614), (735, 0.004173611015886935), (736, 0.00886431343092269), (737, 0.004131889837384435), (738, 0.0027995778819391286), (739, 0.004730009057296232), (740, 0.0027603855532407203), (741, 0.0020996727752386995), (742, 0.0015179202654140936), (743, 0.0001809481281374997), (744, 0.0016736594208072786), (745, 0.002534922639808213), (746, 0.002036796184990325), (747, 0.005798450982740233), (748, 0.012184547509159852), (749, 0.001211130064202293), (750, 0.005021499812552176), (751, 0.0007437841988580244), (752, 0.005696413352503477), (753, 0.0038878328320100797), (754, 0.0013182658679773806), (755, 0.0024561030706918615), (756, 0.004813853950068197), (757, 0.0011495519122848375), (758, 0.0015227795927035004), (759, 0.006433607237841931), (760, 0.004142807322609117), (761, 0.0060775913370250114), (762, 0.0024431062152081686), (763, 0.006662534831116611), (764, 0.0009029155955177102), (765, 0.008288039973755134), (766, 0.008795715952977295), (767, 0.0020784214405569585), (768, 0.00023362081175964418), (769, 0.004996965616390439), (770, 0.0073540213472545034), (771, 0.002782656360812052), (772, 0.0008463199557590024), (773, 0.0018839972016131469), (774, 0.005762311153920789), (775, 0.0015819695154010497), (776, 0.006241485348938414), (777, 0.003843126241898761), (778, 0.00656024165742503), (779, 0.006949259327504824), (780, 0.003715073267608982), (781, 0.009132399365725203), (782, 0.002680969350437986), (783, 0.0017350590736414917), (784, 0.0035385165160241707), (785, 0.0004318920472888697), (786, 0.0011495519122848375), (787, 0.001791625506094595), (788, 0.006269687793826584), (789, 0.0010907968789906383), (790, 0.001744643712568937), (791, 0.00557655161595022), (792, 0.0022299847742811565), (793, 0.0021878040908344452), (794, 0.0018204266835586188), (795, 0.002372624500056411), (796, 0.001148941182187929), (797, 0.003045247314094215), (798, 0.0031999829254611097), (799, 0.001496244907218826), (800, 0.002492682312377828), (801, 0.006863017579869494), (802, 0.009404531690739876), (803, 0.00812964027403903), (804, 0.0026485699760950086), (805, 0.005414888764740754), (806, 0.0008289158608353387), (807, 0.005168593282436694), (808, 0.00114575613715288), (809, 0.0027315153598488607), (810, 0.001343985010354193), (811, 0.0018233938817994433), (812, 0.0031706845326691306), (813, 0.001205691517962174), (814, 0.0030059224767105476), (815, 0.003106772020774026), (816, 0.0011101646959835753), (817, 0.002501967386721097), (818, 0.011224067773017892), (819, 0.002305740649272214), (820, 0.002241696241768022), (821, 0.0021248503290227875), (822, 0.0019503643737634872), (823, 0.003492929368065079), (824, 0.0026588691274783446), (825, 0.005934794649191448), (826, 0.002285433211911962), (827, 0.0036639219345393306), (828, 0.004949062551139832), (829, 0.003891486499758776), (830, 0.005447337753413767), (831, 0.005480499963475241), (832, 0.004813853950068197), (833, 0.0077498817493309525), (834, 0.0019471369504597633), (835, 0.003510999282474801), (836, 0.007246239760803211), (837, 0.00944723445516566), (838, 0.002381197363001422), (839, 0.0027770577650778716), (840, 0.0023683011412696277), (841, 0.007079567549773382), (842, 3.2102852112503136e-05), (843, 0.0017794872001858687), (844, 0.0015010348154295085), (845, 0.001050275831284815), (846, 0.005596677766315492), (847, 0.0004468282250729977), (848, 0.010178233569371548), (849, 0.005735605681201885), (850, 0.0021541487218147666), (851, 0.0029422497454785128), (852, 0.01868019224723447), (853, 0.0035855240506405977), (854, 0.0004963776808563228), (855, 0.0038228919687242143), (856, 0.003510999282474801), (857, 0.0017424416043287667), (858, 0.00790226282548819), (859, 0.001670972296643442), (860, 0.002936027799262756), (861, 0.0029994329190967768), (862, 0.001132545622824513), (863, 0.0031244562190064894), (864, 0.002081942608594248), (865, 0.001850364566911007), (866, 0.0008459124747826412), (867, 0.004163885217188496), (868, 0.0015950572835145439), (869, 0.0019073541138815713), (870, 0.0003520626089782968), (871, 0.027561814764082704), (872, 0.02668229701596056), (873, 0.03246585978530055), (874, 0.0018778261508054404), (875, 0.00279391626246612), (876, 0.005021499812552176), (877, 0.00378972975866274), (878, 0.0007850033036064577), (879, 0.0014443143417547414), (880, 0.0064289571673278355), (881, 0.007395495236485372), (882, 0.012286850175299154), (883, 0.005364500748824636), (884, 0.00959994877638333), (885, 0.001572253527246644), (886, 0.0025776080298744774), (887, 0.05321529929290358), (888, 0.04574108059878953), (889, 0.02294242272480754), (890, 0.0005649428903846178), (891, 0.0014102688670455186), (892, 0.0006064610397733464), (893, 0.0028205377340910372), (894, 0.003283451068422507), (895, 0.001676351339129167), (896, 0.0011629128329055066), (897, 0.009566459778266864), (898, 0.002757947408887842), (899, 0.0019215631209493805), (900, 0.0024336833443838334), (901, 0.003903228347493489), (902, 0.000631864897804858), (903, 0.0006770302238325548), (904, 0.003466223895346174), (905, 0.003653896516795931), (906, 4.4377058624130234e-05), (907, 1.1534641702298518e-05), (908, 0.0022068310746192277), (909, 0.006863017579869494), (910, 0.001969875246747211), (911, 0.0064289571673278355), (912, 0.0049256695272139085), (913, 0.0028798432185130076), (914, 0.0021504551586013668), (915, 0.0060909142709770344), (916, 0.0024651650788284573), (917, 0.0036170745538865222), (918, 0.0005089183175543436), (919, 0.007409149724841574), (920, 0.00025168674132049294), (921, 0.0010753935070234877), (923, 0.0029129223154597214), (924, 0.0009504543183985608), (925, 0.012734728265873399), (926, 0.0013273301457666531), (927, 0.0013396679263812578), (928, 0.0016182109831764729), (929, 0.0011240847136102022), (930, 0.011224067773017892), (931, 0.001632377132297874), (932, 0.0053215299292903575), (933, 0.003653896516795931), (934, 0.004163885217188496), (935, 0.001709034634785988), (936, 0.0057457104693935295), (937, 0.0004854067783123198), (938, 0.0015640428905715077), (939, 0.0032260698476640026), (940, 1.2421921833244558e-05), (941, 0.0011860961799555912), (942, 0.003152585677414119), (943, 0.003397237259366789), (944, 0.0030387956685125057), (945, 0.002608062313149588), (946, 0.0020041589487780906), (947, 0.002649309339964948), (948, 0.002534922639808213), (949, 0.008086565162078861), (950, 0.004972823984253081), (951, 0.0030280793958921983), (952, 0.004730009057296232), (953, 0.00152034797280703), (954, 0.0029356409186405185), (955, 3.282936484500347e-05), (956, 0.0039161379593174945), (957, 0.005046439550933583), (958, 0.004771349172576808), (959, 0.00031773201646045695), (960, 0.0036170745538865222), (961, 0.001695330455024185), (962, 0.0031490781517218864), (963, 0.0008399553198518054), (964, 0.0010550432369274107), (965, 0.0015572402708478599), (966, 0.0011860961799555912), (967, 0.004556833486121144), (968, 0.0025113099541510114), (969, 0.01198979350957235), (970, 0.013408095091369218), (971, 0.002936027799262756), (972, 0.0008275906455926072), (973, 0.005071798464152855), (974, 0.0004424898267429449), (975, 0.004145669109583152), (976, 0.0036439372739091986), (977, 0.004187778442223844), (978, 0.0033638918178011496), (979, 0.0007525225620041175), (980, 0.0021109838088819765), (981, 0.008086565162078861), (982, 0.01596458978787107), (983, 0.0013457359192456033), (984, 0.005021499812552176), (985, 0.003032163482804515), (986, 0.005735605681201885), (987, 0.009566459778266864), (988, 0.0025382561581000704), (989, 0.00046185583634932687), (990, 0.0011157288810941344), (991, 0.0010556231014505204), (992, 0.001709034634785988), (993, 0.019987862465561754), (994, 0.007474791370974179), (995, 0.003891486499758776), (996, 0.002583286389959613), (997, 0.0036375371452908637), (998, 0.031256218943824056), (999, 0.00459820764913935), (1000, 0.03790422135318528), (1001, 0.0013858892422083443), (1002, 0.006495851921919564), (1003, 0.006630431979004109), (1004, 0.003939012312896105), (1005, 0.0026828277609210814), (1006, 0.0017794872001858687), (1007, 0.0071381079582262695), (1008, 0.011224067773017892), (1009, 0.004575122435127926), (1010, 0.0003282880086772827), (1011, 0.00235053491346272), (1012, 0.005824156154498645), (1013, 0.009715181840424273), (1014, 0.001408011843708874), (1015, 0.001048497610903158), (1016, 0.0010483359267216478), (1017, 0.0033152159895020543), (1018, 0.004996965616390439), (1019, 0.007293719504711357), (1020, 0.001182207466928793), (1021, 0.011224067773017892), (1022, 0.008795715952977295), (1023, 0.0033072538611612894), (1024, 0.0073540213472545034), (1025, 0.0025931479925115812), (1026, 0.004835563962309083), (1027, 0.0009660832780884903), (1028, 9.260432626285108e-05), (1029, 0.004650579148857133), (1030, 0.013562879769696456), (1031, 0.0009881546944398177), (1032, 0.002643446644112259), (1033, 0.008467317676952269), (1034, 0.005514407453778687), (1035, 0.0009807326552525198), (1036, 0.0032298615219976718), (1037, 0.006367364132936699), (1038, 0.003222336220516769), (1039, 0.003120742674469207), (1040, 0.0039756656089058335), (1041, 0.00016809172686627384), (1042, 0.0002345870711024628), (1043, 0.00023465673340637466), (1044, 0.007242205770108791), (1045, 0.0005112926541053217), (1046, 0.0021541487218147666), (1047, 0.002424807908090264), (1048, 0.0024913251847820035), (1049, 0.0027714798666025092), (1050, 0.005656418196295046), (1051, 0.005205409585118467), (1052, 0.0012825649165121842), (1053, 0.005696413352503477), (1054, 0.0010093850226561417), (1055, 0.002923083273432278), (1056, 0.003835259413535188), (1057, 0.0030521481434346463), (1058, 0.0021865816354906665), (1059, 0.008808083397788269), (1060, 0.0005327192067369244), (1061, 0.006704047545684609), (1062, 0.002534922639808213), (1063, 0.004468573250132344), (1064, 0.005480499963475241), (1065, 0.009341223533477341), (1066, 0.0005376152849197345), (1067, 0.0012422327170691968), (1068, 0.0025816366092120758), (1069, 0.0007823659848513202), (1070, 0.0009980162706620455), (1071, 0.003372159344345404), (1072, 0.000307378943525547), (1073, 0.0020854720132894205), (1074, 0.0018839972016131469), (1075, 0.0018273452312368395), (1076, 0.00011620287593302767), (1077, 0.001951287764134531), (1078, 0.0038341100116214533), (1079, 0.0030261530758362077), (1080, 0.0014701688174895645), (1081, 0.00035318315846848776), (1082, 0.003492929368065079), (1083, 0.0038341100116214533), (1084, 0.0024292391001715413), (1085, 0.002834120531089753), (1086, 0.002695480070056606), (1087, 0.0014422734012459494), (1088, 0.002474281842633408), (1089, 0.00024248297857694636), (1090, 0.008086565162078861), (1091, 0.0018845450289449854), (1092, 0.0016001719318671191), (1093, 0.0017312235614315285), (1094, 0.006011844953421095), (1095, 0.006863017579869494), (1096, 0.008086565162078861), (1097, 0.011316426684076534), (1098, 0.0002386225588449493), (1099, 0.0064289571673278355), (1100, 0.003021786529345357), (1101, 0.021525516686190214), (1102, 0.0014631346797181807), (1103, 0.002121227710669831), (1104, 0.007451029937860928), (1105, 0.009577814456201018), (1106, 0.0006803766820513874), (1107, 0.00030899786866022007), (1108, 0.002226101066038559), (1109, 0.00834722203177387), (1110, 0.005735605681201885), (1111, 0.002618087730892987), (1112, 0.0017312235614315285), (1113, 0.006630431979004109), (1114, 0.0074932353760359515), (1115, 0.005735605681201885), (1116, 0.003079823101732504), (1117, 0.011169195550715529), (1118, 0.004575122435127926), (1119, 0.003192604818733673), (1120, 0.0012804909370527178), (1121, 0.0007354571714481927), (1122, 0.005447337753413767), (1123, 0.004401503549767425), (1124, 0.005620956638774268), (1125, 0.0032054715448973314), (1126, 0.0033152159895020543), (1127, 0.0018870922177121323), (1128, 0.0077498817493309525), (1130, 0.010451924764473347), (1131, 0.006949259327504824), (1132, 0.004612379138391923), (1133, 0.004104703159169762), (1134, 0.001436148136762366), (1135, 0.0012942050955477662), (1136, 0.0005788719243825136), (1137, 0.0003313193290907584), (1138, 0.0038228919687242143), (1139, 0.0017651310517349738), (1140, 0.0009943872264454054), (1141, 0.0009722608765710904), (1142, 0.0021652839739731877), (1143, 0.004202080158963513), (1144, 0.002224034290699431), (1145, 0.0013140216462781175), (1146, 0.00426068246924523), (1147, 0.009041815014928456), (1148, 0.0051505373628161055), (1149, 0.002573488824239349), (1150, 0.007121696746132938), (1151, 0.05304345583203287), (1152, 0.011224067773017892), (1153, 0.008288039973755134), (1154, 0.016062411488483554), (1155, 0.012115333739350804), (1156, 0.0077498817493309525), (1157, 0.0037466176880179758), (1158, 0.014484411540217582), (1159, 0.010301074725632211), (1160, 0.005658213342038267), (1161, 0.0006077874748646585), (1162, 0.003375145098183076), (1163, 0.004612379138391923), (1164, 0.004839104771496004), (1165, 0.0030255603220721273), (1166, 0.0015777186748790106), (1167, 0.019132919556533727), (1168, 0.010178233569371548), (1169, 0.006505330556531142), (1170, 0.003845412085585472), (1171, 0.0035291793755105835), (1172, 0.00448581709289855), (1173, 0.0029737502595359147), (1174, 0.002501967386721097), (1175, 0.005480499963475241), (1176, 0.004538763571711422), (1177, 0.009805267325348007), (1178, 0.0026836498486914988), (1179, 0.0007243552801250619), (1180, 0.0025239046066425565), (1181, 0.0021915903407992232), (1182, 0.012236574411738437), (1183, 0.004813853950068197), (1184, 0.004813853950068197), (1185, 0.004771349172576808), (1186, 0.03652959746290081), (1187, 0.0073540213472545034), (1188, 0.002301657237255638), (1189, 0.0027770577650778716), (1190, 0.0022183635115470427), (1191, 0.005216124626299176), (1192, 0.007690824171170944), (1193, 0.007133089869491159), (1194, 0.01538252681436262), (1195, 0.003260343767199692), (1196, 0.003821264993465387), (1197, 0.0022068310746192277), (1198, 0.007365282659173208), (1199, 0.005383122963681491), (1200, 0.0012537854643338133), (1201, 0.002751619988400413), (1202, 0.002992971154106106), (1203, 0.006863017579869494), (1204, 0.0011213136616314396), (1205, 0.0034750733038178844), (1206, 0.0018115599402008028), (1207, 0.014228261480562164), (1208, 0.004520907507464228), (1209, 0.00272218554277551), (1210, 1.3317036284073263e-05), (1211, 0.001396031077480026), (1212, 0.003134046551283626), (1213, 0.0017537436782927858), (1214, 0.0016156207339236698), (1215, 9.760081440406438e-05), (1216, 0.0028692280854457864), (1217, 0.0009688854393261934), (1218, 0.0017852681261052413), (1219, 0.0018350186907427927), (1220, 7.6328540833504e-05), (1221, 0.0011438618245133523), (1222, 0.0011841505706348138), (1223, 0.0022106654345460724), (1224, 5.8594959649922355e-05), (1225, 0.0008557093066409134), (1226, 0.006577444112061198), (1227, 0.01657607994751027), (1228, 0.009566459778266864), (1229, 0.00278827580797511), (1230, 0.002706105605654185), (1231, 0.0004977739268211459), (1232, 0.0035201160462797506), (1233, 0.0016156207339236698), (1234, 0.005696413352503477), (1235, 0.00657477102239767), (1236, 0.0023429973525362123), (1237, 0.005644878451301512), (1238, 0.0002481888404281614), (1239, 0.00448581709289855), (1240, 0.00761065300749538), (1241, 0.0034715166918000083), (1242, 0.005350056030576118), (1243, 0.010195181768006469), (1244, 0.0061008605509387135), (1245, 0.0035385165160241707), (1246, 0.006781439884848228), (1247, 0.0077498817493309525), (1248, 0.007474791370974179), (1249, 0.000946922851463364), (1250, 0.003074904372626539), (1251, 0.006308187143608161), (1252, 0.007209489890421581), (1253, 0.0006171053154479219), (1254, 0.030534700708114648), (1255, 0.002322185714730436), (1256, 0.002861366184587492), (1257, 0.0017998180924660894), (1258, 0.0020757441506111577), (1259, 0.002121227710669831), (1260, 0.001037788044238798), (1261, 0.0016707047064069666), (1262, 0.004011938054533396), (1263, 0.0010645775480244831), (1264, 4.2677363257457024e-05), (1265, 0.0031137411778257807), (1266, 0.011224067773017892), (1267, 0.025561876723861565), (1268, 0.06517985193097912), (1269, 0.0004696622543080474), (1270, 0.009559276856002146), (1271, 0.003903228347493489), (1272, 0.015423328447116571), (1273, 0.013809943423533828), (1274, 0.0070407309584325185), (1275, 0.008228286624760751), (1276, 0.003466223895346174), (1277, 0.010178233569371548), (1278, 0.0005788719243825136), (1279, 0.009566459778266864), (1280, 0.001937495896447698), (1281, 0.011224067773017892), (1282, 0.007532470033509091), (1283, 0.0019922055386935207), (1284, 0.00034684567671268017), (1285, 0.0006278252171609354), (1286, 0.0025301704474728175), (1287, 0.001514262604617963), (1288, 0.00018821715199780523), (1289, 0.0008060429647138454), (1290, 0.001793992114647715), (1291, 0.0010862875602653704), (1292, 0.0036408533671172375), (1293, 0.001376372117799036), (1294, 0.0009306011396053992), (1295, 0.0013657576799244303), (1296, 0.001838318037296504), (1297, 0.0006038116627639196), (1298, 0.0040059294536555), (1299, 0.009450967978082064), (1300, 0.0018052519661349426), (1301, 0.0021690140619525437), (1302, 0.0012456625923910017), (1303, 0.0011062245668573623), (1304, 0.0021981122761912425), (1305, 0.008086565162078861), (1306, 0.014069314432666628), (1307, 0.006092273754579926), (1308, 0.004013231528242067), (1309, 0.02113290153361221), (1310, 0.0021248503290227875), (1311, 0.004813853950068197), (1312, 0.0033312674155583054), (1313, 0.004389404039154542), (1314, 0.001966471930667706), (1315, 0.008520625574620522), (1316, 0.0025239046066425565), (1317, 0.00436905456109441), (1318, 0.00403880535775245), (1319, 0.00272218554277551), (1320, 0.011224067773017892), (1321, 0.031897834402848005), (1322, 0.009132399365725203), (1323, 0.016463373330098383), (1324, 0.0006337150988546728), (1325, 0.0023898192140005365), (1326, 0.006630431979004109), (1327, 0.0006669564296317629), (1328, 0.0018839972016131469), (1329, 0.001401671527044438), (1330, 0.0003133586226182291), (1331, 0.005994896754786175), (1332, 0.0034486792811608134), (1333, 0.005071798464152855), (1334, 0.0034750733038178844), (1335, 8.018581135449052e-05), (1336, 0.003192604818733673), (1337, 0.001335194883219684), (1338, 0.005160264927819322), (1339, 0.0035760046511191606), (1340, 0.004835563962309083), (1341, 0.011224067773017892), (1342, 0.000300313989614857), (1343, 0.003878933379096666), (1344, 0.0009272394318376074), (1345, 0.005948467866320346), (1346, 0.00024622430282615507), (1347, 0.001203720492444524), (1348, 0.0025492696875523816), (1349, 0.0017537436782927858), (1350, 0.0049256695272139085), (1351, 0.0023514030557204454), (1352, 0.004080442860183497), (1353, 0.005735605681201885), (1354, 0.0014772359021622652), (1355, 0.0005251624973558728), (1356, 0.0006825978309589405), (1357, 0.008520625574620522), (1358, 0.004771349172576808), (1359, 0.010178233569371548), (1360, 0.007242205770108791), (1361, 0.0001461139027885246), (1362, 4.8283264274715554e-05), (1363, 0.005859688153714539), (1364, 0.0012076651216695942), (1365, 0.003192604818733673), (1366, 0.0031706845326691306), (1367, 0.00017074422151126944), (1368, 0.0018090603402342884), (1369, 0.004080442860183497), (1370, 0.001435321545268977), (1371, 0.0016286166783454122), (1372, 0.0009213013386123303), (1373, 0.0007481470543871337), (1374, 0.00025532026270842865), (1375, 0.0021237284239393277), (1376, 0.01759143190595459), (1377, 0.011224067773017892), (1378, 0.011224067773017892), (1379, 0.0008524956327696227), (1380, 0.017041251149241043), (1381, 0.0029262693594363613), (1382, 0.0006267172452364583), (1383, 0.003074904372626539), (1384, 0.012827087176932043), (1385, 0.004520907507464228), (1386, 0.004670048061808618), (1387, 0.00843303747263095), (1388, 0.005775843260942577), (1389, 0.001435321545268977), (1390, 0.008248116662970548), (1391, 0.00023643395619874592), (1392, 0.0014445357407398918), (1393, 0.004575122435127926), (1394, 0.004258310192097933), (1395, 0.019457432498793878), (1396, 0.004520907507464228), (1397, 0.014304018604476642), (1398, 0.016753793326073293), (1399, 0.0015874176731719494), (1400, 0.0009688854393261934), (1401, 0.004972823984253081), (1402, 0.002121227710669831), (1403, 0.0008996916203184314), (1404, 0.00723477565071232), (1405, 0.00011548231860026496), (1406, 0.011224067773017892), (1407, 0.007474791370974179), (1408, 0.011224067773017892), (1409, 0.010178233569371548), (1410, 0.004822766071848696), (1411, 0.0017016021605569744), (1412, 0.004308297443629533), (1413, 0.003664534508773356), (1414, 0.002834001119921222), (1415, 0.0051238318900972005), (1416, 0.0053215299292903575), (1417, 0.006493171957060111), (1418, 0.00315458265530076), (1419, 0.0017045132409753676), (1420, 0.004522839315029893), (1421, 0.009288742858921744), (1422, 0.0026485699760950086), (1423, 0.004468573250132344), (1424, 0.0021176137692553673), (1425, 0.00391506228909213), (1426, 0.0009604797361419601), (1427, 0.005620956638774268), (1428, 0.0038341100116214533), (1429, 0.002845652296112433), (1430, 0.001937495896447698), (1431, 0.0008459124747826412), (1432, 0.0010103887342360124), (1433, 0.004021734639299259), (1434, 0.01570795912242695), (1435, 0.011241913277548535), (1436, 0.006143425087649577), (1437, 0.008551391451288028), (1438, 0.005262352939961816), (1439, 0.0012772436544616166), (1440, 0.008520625574620522), (1441, 0.0026664307455458265), (1442, 0.0064289571673278355), (1443, 0.0011667520711581298), (1444, 0.009132399365725203), (1445, 0.0033312674155583054), (1446, 0.0030929295400200043), (1447, 0.0008490234445841334), (1448, 0.001734020262545861), (1449, 0.0011845567961818121), (1450, 0.003380472422229925), (1451, 0.0073540213472545034), (1452, 0.004080442860183497), (1453, 0.005262352939961816), (1454, 0.013726035159738989), (1455, 0.0035385165160241707), (1456, 0.0026558051367508257), (1457, 0.0020714036613045583), (1458, 1.863288274986684e-05), (1459, 0.00014928268161065817), (1460, 0.0015925064463572038), (1461, 0.004575122435127926), (1462, 0.002684928839511512), (1463, 0.0009875444944487492), (1464, 0.001505045124008235), (1465, 0.01598256093617568), (1466, 0.034252154980045836), (1467, 0.002285433211911962), (1468, 0.00023179325979356141), (1469, 0.0011590833389228832), (1470, 0.0095557878460321), (1471, 0.003634043338002769), (1472, 0.002072963860558099), (1473, 0.0014890884361618698), (1474, 0.0022471428066471112), (1475, 0.005584597775357764), (1476, 0.01382961256459725), (1477, 0.015212151722142747), (1478, 0.010178233569371548), (1479, 0.011224067773017892), (1480, 0.002896552609637088), (1481, 0.00400060534728724), (1482, 0.011224067773017892), (1483, 0.001706746530975249), (1484, 0.004996965616390439), (1485, 6.71100199624579e-05), (1486, 0.011224067773017892), (1487, 0.0007308445205286264), (1488, 0.0033393576216394117), (1489, 0.003387716004393066), (1490, 0.004798770396720607), (1491, 0.0022377823100812114), (1492, 0.004792451891664319), (1493, 0.000698015538740013), (1494, 0.0017396292646157622), (1495, 0.001036108404947905), (1496, 0.011224067773017892), (1497, 0.004129116353397108), (1498, 0.0010220727705330943), (1499, 0.0030261530758362077), (1500, 0.004631358254286941), (1501, 0.002664045186822816), (1502, 0.003811756716565794), (1503, 0.0014584634071969048), (1504, 0.0059988658381935536), (1505, 0.026387147858931887), (1506, 0.0062512437887648105), (1507, 0.006704047545684609), (1508, 0.003624213858162274), (1509, 0.013562879769696456), (1510, 0.0011101646959835753), (1511, 0.010178233569371548), (1512, 0.0048799442152677956), (1513, 0.011224067773017892), (1514, 0.000289528955708736), (1515, 0.003035840530828187), (1516, 5.323680785676239e-06), (1517, 0.013204510649302276), (1518, 0.0007221007864539771), (1519, 0.0034839749214911155), (1520, 0.006863017579869494), (1521, 0.006630431979004109), (1522, 0.008086565162078861), (1523, 5.678592838054655e-05), (1524, 8.54556387257751e-05), (1525, 0.020602149451264422), (1526, 0.0021809744573920847), (1527, 0.0026231255312967232), (1528, 0.007908851783515837), (1529, 0.006704047545684609), (1530, 0.013260863958008217), (1531, 0.0011783287506046051), (1532, 0.004767792425739471), (1533, 0.00026184341162104724), (1534, 0.0038007030420786746), (1535, 7.6328540833504e-05), (1536, 0.0008045303632646636), (1537, 0.0009824327341018994), (1538, 0.003560243291265951), (1539, 0.002136348519332288), (1540, 0.004149816941645728), (1541, 0.008086565162078861), (1542, 0.00011862752557996466), (1543, 0.007474791370974179), (1544, 0.0016390946358651687), (1545, 0.00024497548459098705), (1546, 0.0014557882705531484), (1547, 0.011224067773017892), (1548, 0.004104703159169762), (1549, 0.00403880535775245), (1550, 0.0027603855532407203), (1551, 0.0031144805416957197), (1552, 0.0052336126458701875), (1553, 0.0021615630847404772), (1554, 0.0014678204593202592), (1555, 0.002081942608594248), (1556, 0.0012315519501553808), (1557, 0.0013097893298446967), (1558, 0.0028070158012113177), (1559, 0.0012102715395094564), (1560, 0.0038682734292970994), (1562, 0.0032312414678473396), (1563, 0.0013679523273211003), (1564, 0.00035430454077145724), (1565, 0.006143425087649577), (1566, 0.0012136006985708162), (1567, 0.0004631021994979298), (1568, 0.0010133663423404237), (1569, 0.0062512437887648105), (1570, 0.0070407309584325185), (1571, 3.194208471405744e-05), (1572, 0.0002938337227349149), (1574, 0.004245817129107454), (1575, 0.0009175093453713964), (1576, 2.1313532894033883e-05), (1577, 0.004013231528242067), (1578, 0.0004648262892091666), (1579, 0.0022953056621571313), (1580, 0.004689771477555542), (1581, 0.001856424092401563), (1582, 0.0022068310746192277), (1583, 0.003106772020774026), (1584, 0.0021357706648900978), (1585, 0.010560348138839253), (1586, 0.006965981225837308), (1587, 0.009851339054427817), (1588, 0.0054336130005795714), (1589, 0.004546489373863172), (1590, 0.005383122963681491), (1591, 0.004321653181573843), (1592, 0.004857590920212137), (1593, 0.00013668134056232112), (1594, 0.00012862577607145831), (1595, 0.0027801523629306896), (1596, 0.00718244605681759), (1597, 0.0064289571673278355), (1598, 0.0031137411778257807), (1599, 0.005658213342038267), (1600, 0.0062838884771550035), (1601, 0.005817183376223151), (1602, 0.0037818659189727054), (1603, 0.004385191825285756), (1604, 0.00315458265530076), (1605, 0.004568338778110501), (1606, 0.0008789020411206944), (1607, 0.0004648262892091666), (1608, 0.004053133717797725), (1609, 0.0014445357407398918), (1610, 0.00020532422511996355), (1611, 0.002348741723345322), (1612, 0.00090130274681342), (1613, 2.1313532894033883e-05), (1614, 0.007474791370974179), (1615, 0.0073540213472545034), (1616, 0.004064820137019515), (1617, 0.006143425087649577), (1618, 0.0037360294353079855), (1619, 0.000946598854376685), (1620, 0.009627707900136393), (1621, 3.732067040266454e-05), (1622, 0.011224067773017892), (1623, 0.0037360294353079855), (1624, 0.00426068246924523), (1625, 0.006932447790692348), (1626, 0.004051756680356891), (1627, 0.0005283562058956749), (1628, 0.001728432034605284), (1629, 0.002583286389959613), (1630, 0.007474791370974179), (1631, 0.012502487577529621), (1632, 0.010042999625104352), (1633, 0.014949582741948358), (1634, 0.0025191291372668573), (1635, 0.001841320664793302), (1636, 0.0035417142600551346), (1637, 0.00281669133367391), (1638, 0.011224067773017892), (1639, 0.0025527030221681847), (1640, 0.0035996361849321788), (1641, 0.0051271039043579636), (1642, 0.004217274574742573), (1643, 0.011224067773017892), (1644, 0.0052462510625934464), (1645, 0.0001816175337387671), (1646, 0.0020333786980950923), (1647, 0.0024381407178447724), (1648, 0.006037971866746345), (1649, 0.0015625441739273702), (1650, 0.0020224455770904257), (1651, 0.001542374930710083), (1652, 0.002022762316880293), (1653, 0.0009756438820672655), (1654, 0.00656024165742503), (1655, 0.0007022478551734337), (1656, 0.011224067773017892), (1657, 0.0037047033311246877), (1658, 0.0032374245444455664), (1659, 0.028552431832905078), (1660, 0.006518260182820183), (1661, 0.006092273754579926), (1662, 0.010178233569371548), (1663, 0.003106772020774026), (1664, 0.009566459778266864), (1665, 0.0034486792811608134), (1666, 0.004131889837384435), (1667, 0.003811756716565794), (1668, 0.007242205770108791), (1669, 0.0036639219345393306), (1670, 0.003939012312896105), (1671, 0.004520907507464228), (1672, 0.001193303778004454), (1673, 0.0012718308464840832), (1674, 0.010643059858580715), (1675, 0.001902663251873845), (1676, 0.0044346657598288985), (1677, 0.011224067773017892), (1678, 0.0037466176880179758), (1679, 0.006296144574831426), (1680, 0.000713592418536906), (1681, 0.00213942841169769), (1682, 0.0052336126458701875), (1683, 0.00326433272310238), (1684, 0.006781439884848228), (1685, 0.0026485699760950086), (1686, 0.0026130666952470464), (1687, 0.0020784214405569585), (1688, 0.012520833047660804), (1689, 0.013679523273211003), (1690, 0.0037311185108769946), (1691, 0.007606075861071373), (1692, 0.003414807837599938), (1693, 0.002834001119921222), (1694, 0.004131889837384435), (1695, 0.007908851783515837), (1696, 0.007533929862453034), (1697, 0.0043372887600351484), (1698, 0.0004120335070110941), (1699, 0.0009554587004960197), (1700, 0.002451592395227201), (1701, 0.004948563685266816), (1702, 0.010178233569371548), (1703, 0.0015373510637238632), (1704, 0.0008000016360452296), (1705, 0.0012335596947220212), (1706, 0.008288039973755134), (1707, 0.0010423623739379943), (1708, 0.0032450256679229764), (1709, 0.0020441455410661886), (1710, 0.008288039973755134), (1711, 0.0035993584472883565), (1712, 0.004010139050851412), (1713, 0.0025686137963498437), (1714, 0.0021435591312180488), (1715, 0.0011064666063401397), (1716, 0.0009890224263159927), (1717, 0.0029029670070539393), (1718, 0.0034227390464860005), (1719, 0.0007207263316812906), (1720, 0.0024470951644187354), (1721, 0.005514407453778687), (1722, 0.0009881546944398177), (1723, 0.00315458265530076), (1724, 0.0024697165749471913), (1725, 0.0006574425688142895), (1726, 0.0010808307014450558), (1727, 0.012488311097449536), (1728, 0.022448135546035784), (1729, 0.008520625574620522), (1730, 0.007474791370974179), (1731, 0.011224067773017892), (1732, 0.0011422592267519586), (1733, 0.0007178687175331749), (1734, 0.0019042412043786593), (1735, 0.006367364132936699), (1736, 0.0035385165160241707), (1737, 0.003725514968930464), (1738, 0.0035665449347455796), (1739, 0.004556833486121144), (1740, 0.0029422497454785128), (1741, 0.0039756656089058335), (1742, 0.0055429597332050185), (1743, 0.007590511342418453), (1744, 0.001937495896447698), (1745, 0.004949062551139832), (1746, 0.005859688153714539), (1747, 0.004306177970846035), (1748, 0.005994896754786175), (1749, 0.003260343767199692), (1750, 0.0035201160462797506), (1751, 0.0009994185023927335), (1752, 0.0013745554880534878), (1753, 0.0033474914409019805), (1754, 0.003903228347493489), (1755, 0.00656024165742503), (1756, 0.0032298615219976718), (1757, 0.002992971154106106), (1758, 0.0018995361630463276), (1759, 0.0030124400670575903), (1760, 0.0008443593919700698), (1761, 0.0053215299292903575), (1762, 0.002863299512713201), (1763, 0.021726617310326376), (1764, 0.019132919556533727), (1765, 0.004612356558939808), (1766, 0.009850353205267521), (1767, 0.0039638479253759264), (1768, 0.0052336126458701875), (1769, 0.003909352934120833), (1770, 0.0035665449347455796), (1771, 0.003963348644023241), (1772, 0.0008350743176268006), (1773, 0.003560848373066469), (1774, 0.003529288231481582), (1775, 0.0026537107640977877), (1776, 0.008795715952977295), (1777, 0.002532104719401513), (1778, 0.0022357112482903297), (1779, 0.0011495519122848375), (1780, 0.007242205770108791), (1781, 0.003283616513727554), (1782, 0.0017565824746508), (1783, 0.011224067773017892), (1784, 0.0021357706648900978), (1785, 0.007548009063407723), (1786, 0.003466223895346174), (1787, 0.0018686166421365091), (1788, 0.003670642746628103), (1789, 0.008902191973787173), (1790, 0.002987711286695824), (1791, 0.025290342144672972), (1792, 0.0030521481434346463), (1793, 0.0026588691274783446), (1794, 0.003951131412744095), (1795, 0.009132399365725203), (1796, 0.0070407309584325185), (1797, 0.005514407453778687), (1798, 0.0034486792811608134), (1799, 0.11118814924007718), (1800, 0.006092273754579926), (1801, 0.0012096413103203953), (1802, 0.0007178687175331749), (1803, 0.00391506228909213), (1804, 0.004709756138185673), (1805, 0.009419512276371346), (1806, 0.002782656360812052), (1807, 0.0018655592554384973), (1808, 0.0069172219478166415), (1809, 0.0032622367970116598), (1810, 0.0037311185108769946), (1811, 0.008086565162078861), (1812, 0.014418979780843161), (1813, 0.0014748765274528934), (1814, 0.002911393459084841), (1815, 0.0033072538611612894), (1816, 0.005658213342038267), (1817, 0.006630431979004109), (1818, 0.0034227390464860005), (1819, 0.0008243134582249755), (1820, 0.0028992254913701165), (1821, 0.002706105605654185), (1822, 0.01767197419196699), (1823, 0.0042715413297801956), (1824, 0.00409129133913486), (1825, 0.004650579148857133), (1826, 0.010178233569371548), (1827, 0.006308187143608161), (1828, 0.003177955967558057), (1829, 0.004650579148857133), (1830, 0.005584597775357764), (1831, 0.0006345640395250176), (1832, 0.002322185714730436), (1833, 0.0009138073814015669), (1834, 0.005414888764740754), (1835, 0.0005814778314537593), (1836, 0.008209406318339524), (1837, 0.001902663251873845), (1838, 0.011224067773017892), (1839, 0.0030387956685125057), (1840, 0.002037168336481816), (1841, 0.0026692390639626385), (1842, 0.003557144157797354), (1843, 0.008086565162078861), (1844, 0.0033152159895020543), (1845, 0.0008428079061581016), (1846, 0.00207221680989581), (1847, 0.010064012084543631), (1848, 0.001202333859426136), (1849, 0.0022935234179930685), (1850, 0.0009104012235663304), (1851, 0.0007093282051874496), (1852, 0.007242205770108791), (1853, 0.005658213342038267), (1854, 0.004145669109583152), (1855, 0.001841320664793302), (1856, 0.0024697165749471913), (1857, 0.0010935942080513466), (1858, 0.0006632004311396449), (1859, 0.0016926044925497396), (1860, 0.009132399365725203), (1861, 0.00075984339062741), (1862, 0.0022839354855706613), (1863, 0.0036439372739091986), (1864, 0.00043018964814332686), (1865, 0.0013800173235130207), (1866, 0.0020130347518770757), (1867, 0.001068174259666144), (1868, 0.003283616513727554), (1869, 0.005775843260942577), (1870, 0.001605303977016286), (1871, 0.005414888764740754), (1872, 0.0029986191540877496), (1873, 0.0013945401486836193), (1874, 0.002420389691699831), (1875, 0.008288039973755134), (1876, 0.0008151505566684979), (1877, 0.008209406318339524), (1878, 0.004575122435127926), (1879, 0.005610521651659185), (1880, 0.0017537436782927858), (1881, 0.0033312674155583054), (1882, 0.004025964260506511), (1883, 0.0011000852041363602), (1884, 0.005520771106481441), (1885, 0.0019247362447670376), (1886, 0.0012929777930322222), (1887, 0.0043009103172027335), (1888, 0.0007805327230937456), (1889, 0.002563754469060437), (1890, 0.0021616614028901115), (1891, 0.004375990747438464), (1892, 0.011224067773017892), (1893, 0.004159575381472124), (1894, 0.0009142540694178613), (1895, 0.0007079094746464424), (1896, 0.007058576462963164), (1897, 0.0049394331498943825), (1898, 0.0071381079582262695), (1899, 0.019342255849236333), (1900, 0.010178233569371548), (1901, 0.002435144100725132), (1902, 0.00272218554277551), (1903, 0.003712749649872536), (1904, 0.0018115599402008028), (1905, 0.004468573250132344), (1906, 0.010467225291740375), (1907, 0.0026281802087852538), (1908, 0.010583302665507593), (1909, 0.0014073225392546508), (1910, 0.006941822331507283), (1911, 0.010478788104195237), (1912, 0.0074744665685460366), (1913, 0.003811756716565794), (1914, 0.003414190469401827), (1915, 0.0021689328272238733), (1916, 0.0030699936479652917), (1917, 0.0030059224767105476), (1918, 0.002738438696196083), (1919, 0.0030521481434346463), (1920, 0.002905297209097752), (1921, 0.002179872922487509), (1922, 0.002233878505007749), (1923, 0.0008060429647138454), (1924, 0.0021615630847404772), (1925, 0.0012784198045117294), (1926, 0.0010990975083258626), (1927, 0.0008709817089216852), (1928, 0.003725514968930464), (1929, 0.0006867870603577559), (1930, 0.0036439372739091986), (1931, 0.014899409959429075), (1932, 0.0048799442152677956), (1933, 0.015397889958136301), (1934, 0.008795715952977295), (1935, 0.008770383650571511), (1936, 0.005352012174492379), (1937, 0.011224067773017892), (1938, 0.004813853950068197), (1939, 0.0019104603766491382), (1940, 0.003611662382070841), (1941, 0.004275695725644014), (1942, 0.007451029937860928), (1943, 0.008520625574620522), (1944, 0.0018975797455181302), (1945, 0.0023429973525362123), (1946, 0.002270732385910313), (1947, 0.0039756656089058335), (1948, 0.002242627323262879), (1949, 0.006704047545684609), (1950, 0.0004010139050851412), (1951, 0.0021578513489960583), (1952, 0.00165494928072529), (1953, 0.0036639219345393306), (1954, 0.0019600882730642045), (1955, 0.001838318037296504), (1956, 0.011224067773017892), (1957, 0.03246585978530055), (1958, 0.005414888764740754), (1959, 0.0020679069741794376), (1960, 0.00812964027403903), (1961, 0.004857590920212137), (1962, 0.0077498817493309525), (1963, 0.0037360294353079855), (1964, 0.0022751291414357866), (1965, 0.0024292391001715413), (1966, 0.002608062313149588), (1967, 0.004948563685266816), (1968, 0.0039756656089058335), (1969, 0.0022106654345460724), (1970, 0.00567963092335964), (1971, 0.004110450514749839), (1972, 0.005113679218046918), (1974, 0.0006420408018991746), (1975, 0.002822439225650756), (1976, 0.0001291018055997077), (1977, 0.0027995778819391286), (1978, 0.005414888764740754), (1979, 0.004401503549767425), (1980, 0.009671127924618167), (1981, 0.0024637589121809106), (1982, 0.0007850033036064577), (1983, 0.0015672324916007035), (1984, 0.004619734652598756), (1985, 0.0023721923599111824), (1986, 0.0032758189779275005), (1987, 0.010704024348984757), (1988, 0.0025012167712251568), (1989, 0.004949062551139832), (1990, 0.007658109066504554), (1991, 0.0011061148795900905), (1992, 0.0011571722306000028), (1993, 0.002241696241768022), (1994, 0.005658213342038267), (1995, 0.00378972975866274), (1996, 0.00013236588340674433), (1997, 0.005994896754786175), (1998, 0.008086565162078861), (1999, 0.002983161679336216), (2000, 0.0027995778819391286), (2001, 0.00015461420595389594), (2002, 0.0021176137692553673), (2003, 0.004595169693214863), (2004, 0.009827456933782503), (2005, 0.006609180973678758), (2006, 0.0021284816660805486), (2007, 0.0024751664332483573), (2008, 0.0005898236725271912), (2009, 0.002782656360812052), (2010, 0.006196371566462448), (2011, 0.004131889837384435), (2012, 0.0035385165160241707), (2013, 0.00957590128296162), (2014, 0.004051756680356891), (2015, 0.0023639731998028753), (2016, 0.03215545055809649), (2017, 0.0006895863558159826), (2018, 0.010178233569371548), (2019, 0.00028738116912656997), (2020, 0.0011763931251701945), (2021, 0.0007833535291431086), (2022, 0.001564728225765742), (2023, 0.003177955967558057), (2024, 1.508376222608268e-05), (2025, 5.911860675428915e-05), (2026, 0.006042799820036782), (2027, 0.0013119040040790013), (2028, 0.0026796807652841215), (2029, 0.0028992254913701165), (2030, 0.0028714748701326996), (2031, 0.0029175144403768977), (2032, 0.0002671282500160104), (2033, 0.0014102688670455186), (2034, 0.002499784070522306), (2035, 0.005775843260942577), (2036, 0.000946598854376685), (2037, 0.0016156207339236698), (2038, 0.0009841347306943245), (2039, 0.009132399365725203), (2040, 0.004949062551139832), (2041, 0.0029356409186405185), (2042, 0.011224067773017892), (2043, 0.0011447535590602593), (2044, 0.0019535987154504332), (2045, 0.0009839677533657828), (2046, 0.0060775913370250114), (2047, 0.0015547525212006867), (2048, 0.0005441257107659579), (2049, 0.0001444628345062641), (2050, 0.006932447790692348), (2051, 0.0014745591813179783), (2052, 0.0015325454508695877), (2053, 0.0029737502595359147), (2054, 0.0038166702315253894), (2055, 0.009132399365725203), (2056, 0.009566459778266864), (2057, 0.011224067773017892), (2058, 0.006978975422097183), (2059, 1.77456026189208e-05), (2060, 0.00426068246924523), (2061, 0.014083166103402059), (2062, 0.006662534831116611), (2063, 0.0006867225116851569), (2064, 0.001710529387373918), (2065, 0.005383122963681491), (2066, 0.0016001719318671191), (2067, 0.0003758725057657438), (2068, 0.02116349536214379), (2069, 0.010560348138839253), (2070, 0.0024561030706918615), (2071, 0.0070407309584325185), (2072, 0.0036639219345393306), (2073, 0.0070415830517010294), (2074, 0.010375624291618086), (2075, 0.011224067773017892), (2076, 0.0014376198241888953), (2077, 0.0026231255312967232), (2078, 0.018374543176055132), (2079, 0.0003129673305546849), (2080, 0.003304590486839379), (2081, 0.0029269316462017963), (2082, 0.0010602948702244737), (2083, 0.0024510893935106716), (2084, 0.0013099259916671416), (2085, 0.0015925064463572038), (2086, 0.004876281435689545), (2087, 0.0004977739268211459), (2088, 0.014257558989600483), (2089, 0.0014261632784913739), (2090, 0.01151937287405203), (2091, 0.012813143539897019), (2092, 0.007037706488732997), (2093, 0.0011344257723752066), (2094, 0.0029867654681564344), (2095, 0.001170601103396496), (2096, 0.00774985916987884), (2097, 0.004538763571711422), (2098, 0.004495913881698428), (2099, 0.003951131412744095), (2100, 0.005656418196295046), (2101, 0.0034015943989598283), (2102, 0.004538763571711422), (2103, 0.0016522952434196896), (2104, 0.010178233569371548), (2105, 0.003712848184803126), (2106, 0.004231096818990376), (2107, 0.006949259327504824), (2108, 0.0012395990082656192), (2109, 0.003557144157797354), (2110, 0.008795715952977295), (2111, 0.011224067773017892), (2112, 0.002381197363001422), (2113, 0.005696413352503477), (2114, 0.008795715952977295), (2115, 0.006092273754579926), (2116, 0.0035951038911987247), (2117, 0.002905297209097752), (2118, 0.010178233569371548), (2119, 0.0020757441506111577), (2120, 0.0025929427159623528), (2121, 0.0011860961799555912), (2122, 0.01180685024771696), (2123, 0.00022409000194031673), (2124, 0.014484411540217582), (2125, 0.012857914334655671), (2126, 0.008039042295852365), (2127, 0.004094190672747322), (2128, 0.001969875246747211), (2129, 0.0064289571673278355), (2130, 0.0002709123804364504), (2131, 0.011224067773017892), (2132, 0.005046439550933583), (2133, 0.010178233569371548), (2134, 0.009132399365725203), (2135, 0.03874940874665476), (2136, 0.0064289571673278355), (2137, 0.0014748765274528934), (2138, 0.0014284476389870647), (2139, 0.004835563962309083), (2140, 0.0035855240506405977), (2141, 0.006725088725304066), (2142, 0.0026796807652841215), (2143, 0.008231686665049191), (2144, 0.006781439884848228), (2145, 0.0024561030706918615), (2146, 0.00718244605681759), (2147, 0.0027714798666025092), (2148, 0.0004916179826669265), (2149, 0.003084749861420166), (2150, 0.006704047545684609), (2151, 0.0014414526633625812), (2152, 0.009882006800156466), (2153, 0.006363683132009494), (2154, 0.003845412085585472), (2155, 0.011659286538879281), (2156, 0.008247332928320927), (2157, 0.0007050760093645373), (2158, 0.002359855849924461), (2159, 0.0012368822907767622), (2160, 0.00656024165742503), (2161, 0.009132399365725203), (2162, 0.005775843260942577), (2163, 0.0026153552305347807), (2164, 0.007021998564949602), (2165, 0.00426068246924523), (2166, 0.0008212529707269702), (2167, 0.0007423327126285353), (2168, 0.0019439164160050398), (2169, 0.00041659623878815356), (2170, 0.002954771143640896), (2171, 0.002581778921538323), (2172, 0.0024426112983574845), (2173, 0.0064289571673278355), (2174, 0.0006596492523776216), (2175, 0.006259052937723635), (2176, 0.0019863292791581716), (2177, 0.0018115599402008028), (2178, 0.0034574260872171168), (2179, 0.003845412085585472), (2180, 0.003951131412744095), (2181, 0.0029801300568601677), (2182, 0.006092273754579926), (2183, 0.005480499963475241), (2184, 0.0024788609654779973), (2185, 0.002738438696196083), (2186, 0.003963348644023241), (2187, 0.0022299847742811565), (2188, 0.006143425087649577), (2189, 0.01034603784348244), (2190, 0.006949259327504824), (2191, 0.011224067773017892), (2192, 0.07606075861071374), (2193, 0.037373956854870896), (2194, 0.032346260648315445), (2195, 0.007114288315594708), (2196, 0.009362228023407622), (2197, 0.001062337710253213), (2198, 0.0028515117979200965), (2199, 0.0005749714832120442), (2200, 0.0011605012093389153), (2201, 0.005994896754786175), (2202, 0.006863017579869494), (2203, 0.003903228347493489), (2204, 0.0051777240410307785), (2205, 0.0013645210418337256), (2206, 0.000765226870681443), (2207, 0.0016516920661684236), (2208, 0.0024045291058712275), (2209, 0.001973151729726829), (2210, 0.005696413352503477), (2211, 0.0018204266835586188), (2212, 0.003715073267608982), (2213, 0.008904404264154236), (2214, 0.01207594373349269), (2215, 0.005551093700733021), (2216, 0.009116387005537517), (2217, 0.0022534993253159373), (2218, 0.001408011843708874), (2219, 0.003058868955523419), (2220, 0.0033774375678802793), (2221, 0.004321653181573843), (2222, 0.003120742674469207), (2223, 0.002326325140699061), (2224, 0.004110450514749839), (2225, 0.011964251847374567), (2226, 0.010372591970046325), (2227, 0.005352012174492379), (2228, 0.004840779383399662), (2229, 0.0029737502595359147), (2230, 0.003376138358167714), (2231, 0.004976122229664249), (2232, 0.001244889096127422), (2233, 0.003134843896913292), (2234, 0.01031994866775662), (2235, 0.002520710731099237), (2236, 0.0019648654682037987), (2237, 0.00029755325050725767), (2238, 4.617932127040675e-05), (2239, 0.00018105839918645136), (2241, 0.003963348644023241), (2242, 0.00012077668639952534), (2243, 0.0005483545717687598), (2244, 0.0022653954696156857), (2245, 0.002060581598089469), (2246, 5.731001237088092e-05), (2247, 0.006784258972544839), (2248, 0.0015899599144123013), (2249, 0.00448581709289855), (2250, 0.017156681668369724), (2251, 0.004097766052544418), (2252, 6.662441457707595e-05), (2253, 0.001556682066358197), (2254, 0.00426068246924523), (2255, 0.00021217260625482808), (2256, 0.0003922140293546389), (2257, 0.003086055633738092), (2258, 0.004815911931048858), (2259, 0.004221796959850349), (2260, 0.001544842338977366), (2261, 0.007327843869078661), (2262, 0.0021578513489960583), (2263, 0.00028630842125401294), (2264, 3.3716644975949514e-05), (2265, 0.000537383653123761), (2266, 0.00037320887303941386), (2267, 0.004151152176955192), (2268, 0.006414794576773642), (2270, 0.0045032602908634594), (2271, 0.00015684762666483767), (2272, 0.0018544788636752148), (2273, 0.000371435493018075), (2274, 2.130725805451722e-05), (2275, 0.00028442019543012495), (2277, 0.0011629280043269307), (2278, 0.004173611015886935), (2279, 0.008520625574620522), (2280, 0.005775843260942577), (2281, 0.00036105039322698854), (2282, 0.03542055074315088), (2283, 0.0051238318900972005), (2284, 0.0026692390639626385), (2285, 0.004538763571711422), (2286, 0.001000486708490063), (2287, 0.0003232166456156688), (2288, 0.002588209134356426), (2289, 0.0016576079947510272), (2290, 0.000649496251377842), (2291, 0.011224067773017892), (2292, 0.0027548688384323315), (2293, 0.0013811876737614783), (2294, 0.0023729542731015744), (2295, 0.00409129133913486), (2296, 0.004612379138391923), (2297, 0.00409129133913486), (2298, 0.018264798731450406), (2299, 0.004636587630402429), (2300, 0.0023984906161394564), (2301, 0.0021471709085952194), (2302, 0.000588012357991435), (2303, 0.0017509102130108805), (2304, 0.0020299689343406674), (2305, 0.00354780159036744), (2306, 0.0028052608258295926), (2307, 0.004902633662674003), (2308, 0.0016260085189765273), (2309, 0.008520625574620522), (2310, 0.009132399365725203), (2311, 0.0061008605509387135), (2312, 0.0017766050220044127), (2313, 0.0036047449452107903), (2314, 0.005994896754786175), (2315, 0.0053215299292903575), (2316, 0.0014819657487289912), (2317, 0.017616166795576538), (2318, 0.011224067773017892), (2319, 0.0023206972613207355), (2320, 0.005013262878670321), (2321, 0.0017939812580334768), (2322, 0.036437867095829274), (2323, 0.002240074101117326), (2324, 0.0010808307014450558), (2325, 0.004762394726002844), (2326, 0.0021578513489960583), (2327, 0.0027623753475229567), (2329, 0.008194546079546583), (2330, 0.0028429930840178277), (2331, 0.004750537534771031), (2332, 0.0036961641724034713), (2333, 0.003372159344345404), (2334, 0.0015522688666165122), (2335, 0.003144507054493288), (2336, 0.004159575381472124), (2337, 0.002911393459084841), (2338, 0.005859688153714539), (2339, 2.1440073995388316e-05), (2340, 0.002322185714730436), (2341, 0.004750537534771031), (2342, 0.011224067773017892), (2343, 0.008130042594882637), (2344, 0.004117053666114236), (2345, 0.0017007971994799141), (2346, 0.004173611015886935), (2347, 0.004792451891664319), (2348, 0.009132399365725203), (2349, 0.008086565162078861), (2350, 0.007474791370974179), (2351, 0.0005814778314537593), (2352, 0.014276215916452539), (2353, 0.005775843260942577), (2354, 0.006582909112777611), (2355, 0.00812964027403903), (2356, 0.0005348858745638386), (2357, 0.006966557144191307), (2358, 0.0013967770947111414), (2359, 0.0012375832166241787), (2360, 0.0021327496642386044), (2361, 0.0038341100116214533), (2362, 0.008520625574620522), (2363, 0.008086565162078861), (2364, 0.0002684400798498316), (2365, 0.007234463602380589), (2366, 0.00757945951732548), (2367, 0.008795715952977295), (2368, 0.00010608630312741404), (2369, 0.005514407453778687), (2370, 0.0013016643672228796), (2371, 0.003222336220516769), (2372, 0.0051505373628161055), (2373, 0.003891486499758776), (2374, 0.006143425087649577), (2375, 0.0019219078243592404), (2376, 0.0008379428943508169), (2377, 0.0003255619576919255), (2378, 0.0012804909370527178), (2379, 0.0016977284252263464), (2380, 0.005817183376223151), (2381, 0.0008562889679181108), (2382, 0.001983024086556747), (2383, 0.00044454473975281646), (2384, 0.006630431979004109), (2385, 0.0033152159895020543), (2386, 0.005658213342038267), (2387, 0.005514407453778687), (2388, 0.02128611971716143), (2389, 0.008520625574620522), (2390, 0.009301158297714267), (2391, 0.0033072538611612894), (2392, 0.004792451891664319), (2393, 0.0018285081388357226), (2394, 0.0022935234179930685), (2395, 0.0019471369504597633), (2396, 0.0021321217639117137), (2397, 0.0025377374243479235), (2398, 0.00045008350342600644), (2399, 0.0034905183916926785), (2400, 0.0021727533942733942), (2401, 0.007474791370974179), (2402, 0.0051777240410307785), (2403, 0.008103513360713781), (2404, 0.00010101829087480402), (2405, 0.00563338266734782), (2406, 0.007759670647135131), (2407, 0.004893688062029495), (2408, 0.0019763093888796354), (2409, 0.0033152159895020543), (2410, 0.00732737875252334), (2411, 0.0029236603544517833), (2412, 0.028804513820011365), (2413, 0.012792730583470281), (2414, 0.004467757010015498), (2415, 0.0043372887600351484), (2416, 0.006666681986449614), (2417, 0.0070407309584325185), (2418, 0.0060511206441442545), (2419, 0.003174835346343899), (2420, 0.0033888315561825554), (2421, 0.009132399365725203), (2422, 0.006551637955855001), (2423, 0.010178233569371548), (2424, 0.0018293458414672496), (2425, 0.005726599025426402), (2426, 0.01483698662297862), (2427, 0.004857590920212137), (2428, 0.0064289571673278355), (2429, 0.003072401062545206), (2430, 0.003501820426021761), (2431, 0.0032914545563888053), (2432, 0.0027511256063952674), (2433, 0.024387824450437807), (2434, 0.0001118631047862154), (2435, 0.009196859515379702), (2436, 0.004156842881113917), (2437, 0.0005768141012505528), (2438, 0.002826451172680945), (2439, 0.003557144157797354), (2440, 0.004264243527823427), (2441, 0.00272218554277551), (2442, 0.0008521408420137505), (2443, 0.003185262615344033), (2444, 6.972172555981659e-05), (2445, 0.005291651332753796), (2446, 0.0007022478551734337), (2447, 0.00010669340814364257), (2448, 0.0015076360231559883), (2449, 0.0018649547005562847), (2450, 0.004595169693214863), (2451, 3.197029934105083e-05), (2452, 0.00013944345111963319), (2453, 0.0060775913370250114), (2454, 0.0005492230798682008), (2455, 0.00011548231860026496), (2456, 0.00012833530485200384), (2457, 0.004867366688767667), (2458, 0.004385191825285756), (2459, 0.0001138063020198854), (2460, 0.004857590920212137), (2461, 0.00041496303204247405), (2462, 0.0006853893577851853), (2463, 0.001040291270959186), (2464, 0.0015154964579543908), (2465, 0.009566459778266864), (2466, 0.002863299512713201), (2467, 0.0032993335289622804), (2468, 0.004857590920212137), (2469, 0.0006391596561542321), (2470, 0.007242205770108791), (2471, 0.0027081208953302193), (2472, 0.004520907507464228), (2473, 0.006120664290275245), (2474, 0.004677623993357131), (2475, 0.0032993335289622804), (2476, 7.541881113041338e-05), (2477, 0.0033072538611612894), (2478, 0.0017737314595609904), (2479, 0.0012862577607145832), (2480, 0.011224067773017892), (2481, 0.007474791370974179), (2482, 0.0017993832406368628), (2483, 0.008520625574620522), (2484, 0.002823308756636589), (2485, 0.0027471482414310236), (2486, 0.0012284115626066295), (2487, 0.0010881109077007934), (2488, 0.006678715243278823), (2489, 0.011224067773017892), (2490, 0.0251565559391193), (2491, 0.004205014581133314), (2492, 0.008520625574620522), (2493, 0.005071798464152855), (2494, 0.0009272394318376074), (2495, 0.01147121136240377), (2496, 0.00014259478316889314), (2497, 0.005291651332753796), (2498, 0.0016390946358651687), (2499, 0.0001963005498370964), (2500, 0.005554115530155743), (2501, 0.002821485226535485), (2502, 0.0033743345962563426), (2503, 0.0028714748701326996), (2504, 0.008795715952977295), (2505, 0.0044346657598288985), (2506, 0.003951131412744095), (2507, 0.00656024165742503), (2508, 0.019132919556533727), (2509, 0.0001018186274322224), (2510, 0.00012124148928847318), (2511, 0.0006144371983614986), (2512, 0.0013692014056843806), (2513, 0.0011194497703268449), (2514, 0.011224067773017892), (2515, 0.017041251149241043), (2516, 0.013408095091369218), (2517, 5.0574967463924274e-05), (2518, 0.008795715952977295), (2519, 0.0077498817493309525)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SbJYRgTTWJtK"
      },
      "source": [
        "### NAIL BITTING"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YQBjR9X0ZqQY"
      },
      "source": [
        "import logging\n",
        "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
        "import os\n",
        "import gensim\n",
        "# Set file names for train and test data\n",
        "test_data_dir = os.path.join(gensim.__path__[0], 'test', 'test_data')\n",
        "lee_train_file = os.path.join(test_data_dir, 'lee_background.cor')\n",
        "lee_test_file = os.path.join(test_data_dir, 'lee.cor')\n",
        "\n",
        "import smart_open\n",
        "\n",
        "def read_corpus(fname, tokens_only=False):\n",
        "    with smart_open.open(fname, encoding=\"iso-8859-1\") as f:\n",
        "        for i, line in enumerate(f):\n",
        "            tokens = gensim.utils.simple_preprocess(line)\n",
        "            if tokens_only:\n",
        "                yield tokens\n",
        "            else:\n",
        "                # For training data, add tags\n",
        "                yield gensim.models.doc2vec.TaggedDocument(tokens, [i])\n",
        "\n",
        "train_corpus = list(read_corpus(lee_train_file))\n",
        "test_corpus = list(read_corpus(lee_test_file, tokens_only=True))\n",
        "\n",
        "###############################################################################\n",
        "# Let's take a look at the training corpus\n",
        "#\n",
        "print(train_corpus[:2])\n",
        "\n",
        "###############################################################################\n",
        "# And the testing corpus looks like this:\n",
        "#\n",
        "print(test_corpus[:2])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hIoTtbgDWOKq"
      },
      "source": [
        "model = gensim.models.doc2vec.Doc2Vec(vector_size=50, min_count=2, epochs=40)\n",
        "#gensim.models.KeyedVectors.load_word2vec_format(\"GoogleNews-vectors-negative300.bin.gz\", binary=True)\n",
        "# Build a vocabulary\n",
        "model.build_vocab(train_corpus)\n",
        "\n",
        "\n",
        "#print(f\"Word 'penalty' appeared {model.wv.get_vecattr('penalty', 'count')} times in the training corpus.\")\n",
        "\n",
        "\n",
        "model.train(train_corpus, total_examples=model.corpus_count, epochs=model.epochs)\n",
        "\n",
        "\n",
        "vector = model.infer_vector(['only', 'you', 'can', 'prevent', 'forest', 'fires'])\n",
        "print(vector)\n",
        "\n",
        "ranks = []\n",
        "second_ranks = []\n",
        "for doc_id in range(len(train_corpus)):\n",
        "    inferred_vector = model.infer_vector(train_corpus[doc_id].words)\n",
        "    sims = model.docvecs.most_similar([inferred_vector], topn=len(model.docvecs))\n",
        "    rank = [docid for docid, sim in sims].index(doc_id)\n",
        "    ranks.append(rank)\n",
        "\n",
        "    second_ranks.append(sims[1])\n",
        "\n",
        "\n",
        "import collections\n",
        "\n",
        "counter = collections.Counter(ranks)\n",
        "print(counter)\n",
        "\n",
        "print('Document ({}): «{}»\\n'.format(doc_id, ' '.join(train_corpus[doc_id].words)))\n",
        "print(u'SIMILAR/DISSIMILAR DOCS PER MODEL %s:\\n' % model)\n",
        "for label, index in [('MOST', 0), ('SECOND-MOST', 1), ('MEDIAN', len(sims)//2), ('LEAST', len(sims) - 1)]:\n",
        "    print(u'%s %s: «%s»\\n' % (label, sims[index], ' '.join(train_corpus[sims[index][0]].words)))\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jz5dyfdSaBB4"
      },
      "source": [
        "# Pick a random document from the corpus and infer a vector from the model\n",
        "import random\n",
        "doc_id = random.randint(0, len(train_corpus) - 1)\n",
        "\n",
        "# Compare and print the second-most-similar document\n",
        "print('Train Document ({}): «{}»\\n'.format(doc_id, ' '.join(train_corpus[doc_id].words)))\n",
        "sim_id = second_ranks[doc_id]\n",
        "print('Similar Document {}: «{}»\\n'.format(sim_id, ' '.join(train_corpus[sim_id[0]].words)))\n",
        "\n",
        "# Testing the Model\n",
        "\n",
        "# Pick a random document from the test corpus and infer a vector from the model\n",
        "doc_id = random.randint(0, len(test_corpus) - 1)\n",
        "inferred_vector = model.infer_vector(test_corpus[doc_id])\n",
        "sims = model.docvecs.most_similar([inferred_vector], topn=len(model.docvecs))\n",
        "\n",
        "# Compare and print the most/median/least similar documents from the train corpus\n",
        "print('Test Document ({}): «{}»\\n'.format(doc_id, ' '.join(test_corpus[doc_id])))\n",
        "print(u'SIMILAR/DISSIMILAR DOCS PER MODEL %s:\\n' % model)\n",
        "for label, index in [('MOST', 0), ('MEDIAN', len(sims)//2), ('LEAST', len(sims) - 1)]:\n",
        "    print(u'%s %s: «%s»\\n' % (label, sims[index], ' '.join(train_corpus[sims[index][0]].words)))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gmMtNOV5dP6Y"
      },
      "source": [
        "# MY Corpus Based Similairty Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0NXCemMZhxtm"
      },
      "source": [
        "### Creating Custom Corpus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p686zae_hu56",
        "outputId": "4d544a88-e6ac-4e20-de8f-2540e30c8081"
      },
      "source": [
        "import os, os.path\n",
        "path = os.path.expanduser('~/natural_language_toolkit_data')\n",
        "if not os.path.exists(path):\n",
        "   os.mkdir(path)\n",
        "os.path.exists(path)\n",
        "print(path)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/root/natural_language_toolkit_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lEqg7i9Eh_w8",
        "outputId": "78be9949-5ac8-4b51-fa60-628d7329865e"
      },
      "source": [
        "import nltk.data\n",
        "path in nltk.data.path\n",
        "print(path)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/root/natural_language_toolkit_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9RM8MCoLiFu-"
      },
      "source": [
        "import nltk.data\n",
        "nltk.data.load('/root/natural_language_toolkit_data/wordfile.txt', format = 'raw')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "61pj5W3YkXaB"
      },
      "source": [
        "### Training and Building the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NPqQSVnudMOZ"
      },
      "source": [
        "import logging\n",
        "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
        "import os\n",
        "import gensim\n",
        "# Set file names for train and test data\n",
        "test_data_dir = os.path.join(gensim.__path__[0], 'test', 'test_data')\n",
        "lee_train_file = os.path.join(test_data_dir, 'lee_background.cor')\n",
        "lee_test_file = os.path.join(test_data_dir, 'lee.cor')\n",
        "\n",
        "import smart_open\n",
        "\n",
        "def read_corpus(fname, tokens_only=False):\n",
        "    with smart_open.open(fname, encoding=\"iso-8859-1\") as f:\n",
        "        for i, line in enumerate(f):\n",
        "            tokens = gensim.utils.simple_preprocess(line)\n",
        "            if tokens_only:\n",
        "                yield tokens\n",
        "            else:\n",
        "                # For training data, add tags\n",
        "                yield gensim.models.doc2vec.TaggedDocument(tokens, [i])\n",
        "\n",
        "train_corpus = list(read_corpus(lee_train_file))\n",
        "test_corpus = list(read_corpus(lee_test_file, tokens_only=True))\n",
        "\n",
        "###############################################################################\n",
        "# Let's take a look at the training corpus\n",
        "#\n",
        "print(train_corpus[:2])\n",
        "\n",
        "###############################################################################\n",
        "# And the testing corpus looks like this:\n",
        "#\n",
        "print(test_corpus[:2])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nmZuZr-lCcw5"
      },
      "source": [
        "#JUST DOC2VEC CUSTOMIZED"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SkipbVM6CmkP"
      },
      "source": [
        "import nltk\n",
        "import gensim\n",
        "import numpy as np\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize,sent_tokenize\n",
        "\n",
        "file_docs = []\n",
        "#Put the File Path of your 1st Text Document\n",
        "with open ('/content/drive/MyDrive/GoogleCollabWork/nst1.txt') as f:\n",
        "    tokens = sent_tokenize(f.read())\n",
        "    for line in tokens:\n",
        "        file_docs.append(line)\n",
        "\n",
        "print(\"Number of documents:\",len(file_docs))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ThYcILZgI-SG"
      },
      "source": [
        "gen_docs = [[w.lower() for w in word_tokenize(text)] \n",
        "            for text in file_docs]"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aTVg_yl_JDTy"
      },
      "source": [
        "print(gen_docs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z1GkuRWfJPqv"
      },
      "source": [
        "dictionary = gensim.corpora.Dictionary(gen_docs)\n",
        "print(dictionary.token2id)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MCc38S_PKD0O"
      },
      "source": [
        "corpus = [dictionary.doc2bow(gen_doc) for gen_doc in gen_docs]"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b9j-E_i_KFSK"
      },
      "source": [
        "tf_idf = gensim.models.TfidfModel(corpus)\n",
        "for doc in tf_idf[corpus]:\n",
        "    print([[dictionary[id], np.around(freq, decimals=2)] for id, freq in doc])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E3g502NcLJ5U"
      },
      "source": [
        "sims = gensim.similarities.Similarity('/content',tf_idf[corpus],num_features=len(dictionary))"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EztMpKv0LNgZ"
      },
      "source": [
        "file2_docs = []\n",
        "\n",
        "#Put the File Path of your 2nd Text Document\n",
        "with open ('/content/drive/MyDrive/GoogleCollabWork/nst2.txt') as f:\n",
        "    tokens = sent_tokenize(f.read())\n",
        "    for line in tokens:\n",
        "        file2_docs.append(line)\n",
        "\n",
        "print(\"Number of documents:\",len(file2_docs))  \n",
        "for line in file2_docs:\n",
        "    query_doc = [w.lower() for w in word_tokenize(line)]\n",
        "    query_doc_bow = dictionary.doc2bow(query_doc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d_mMf-glLg3x"
      },
      "source": [
        "query_doc_tf_idf = tf_idf[query_doc_bow]\n",
        "print('Comparing Result:', sims[query_doc_tf_idf]) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j_P-mIOXmLoo"
      },
      "source": [
        "sum_of_sims =(np.sum(sims[query_doc_tf_idf], dtype=np.float32))\n",
        "print(sum_of_sims)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NN6oALXrmNyx",
        "outputId": "786d0260-1061-421d-eb6d-d5fec32e88a1"
      },
      "source": [
        "percentage_of_similarity = round(float((sum_of_sims / len(file_docs)) * 100))\n",
        "print(f'Average similarity float: {float(sum_of_sims / len(file_docs))}')\n",
        "print(f'Average similarity percentage: {float(sum_of_sims / len(file_docs)) * 100}')\n",
        "print(f'Average similarity rounded percentage: {percentage_of_similarity}')"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Average similarity float: 0.09743563532829284\n",
            "Average similarity percentage: 9.743563532829285\n",
            "Average similarity rounded percentage: 10\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EHY8WWJt1uqj"
      },
      "source": [
        "print(sims[query_doc_tf_idf*100],\"%\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x7ywcGrXmnOE"
      },
      "source": [
        "avg_sims = [] # array of averages\n",
        "\n",
        "# for line in query documents\n",
        "for line in file2_docs:\n",
        "        # tokenize words\n",
        "        query_doc = [w.lower() for w in word_tokenize(line)]\n",
        "        # create bag of words\n",
        "        query_doc_bow = dictionary.doc2bow(query_doc)\n",
        "        # find similarity for each document\n",
        "        query_doc_tf_idf = tf_idf[query_doc_bow]\n",
        "        # print (document_number, document_similarity)\n",
        "        print('Comparing Result:', sims[query_doc_tf_idf]) \n",
        "        # calculate sum of similarities for each query doc\n",
        "        sum_of_sims =(np.sum(sims[query_doc_tf_idf], dtype=np.float32))\n",
        "        # calculate average of similarity for each query doc\n",
        "        avg = sum_of_sims / len(file_docs)\n",
        "        # print average of similarity for each query doc\n",
        "        print(f'avg: {sum_of_sims / len(file_docs)}')\n",
        "        # add average values into array\n",
        "        avg_sims.append(avg)  \n",
        "# calculate total average\n",
        "total_avg = np.sum(avg_sims, dtype=np.float)\n",
        "# round the value and multiply by 100 to format it as percentage\n",
        "percentage_of_similarity = round(float(total_avg) * 100)\n",
        "# if percentage is greater than 100\n",
        "# that means documents are almost same\n",
        "if percentage_of_similarity >= 100:\n",
        "  percentage_of_similarity = 100"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VbsrdaTfoLx2"
      },
      "source": [
        "#BERT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-MODWqPfoOEw"
      },
      "source": [
        "!pip install sentence-transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "akoAAvxJoKkI"
      },
      "source": [
        "sentences = [\n",
        "    \"Three years later, the coffin was still full of Jello.\",\n",
        "    \"The fish dreamed of escaping the fishbowl and into the toilet where he saw his friend go.\",\n",
        "    \"The person box was packed with jelly many dozens of months later.\",\n",
        "    \"Standing on one's head at job interviews forms a lasting impression.\",\n",
        "    \"It took him a month to finish the meal.\",\n",
        "    \"He found a leprechaun in his walnut shell.\"\n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e0EWdqqmpUoB"
      },
      "source": [
        "from transformers import AutoTokenizer, AutoModel\n",
        "import torch\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/bert-base-nli-mean-tokens')\n",
        "model = AutoModel.from_pretrained('sentence-transformers/bert-base-nli-mean-tokens')\n",
        "\n",
        "# initialize dictionary that will contain tokenized sentences\n",
        "tokens = {'input_ids': [], 'attention_mask': []}\n",
        "\n",
        "for sentence in sentences:\n",
        "    # tokenize sentence and append to dictionary lists\n",
        "    new_tokens = tokenizer.encode_plus(sentence, max_length=128, truncation=True,\n",
        "                                       padding='max_length', return_tensors='pt')\n",
        "    tokens['input_ids'].append(new_tokens['input_ids'][0])\n",
        "    tokens['attention_mask'].append(new_tokens['attention_mask'][0])\n",
        "\n",
        "# reformat list of tensors into single tensor\n",
        "tokens['input_ids'] = torch.stack(tokens['input_ids'])\n",
        "tokens['attention_mask'] = torch.stack(tokens['attention_mask'])\n",
        "tokens['input_ids'].shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NbF9YtptpdJl"
      },
      "source": [
        "outputs = model(**tokens)\n",
        "outputs.keys()\n",
        "\n",
        "embeddings = outputs.last_hidden_state\n",
        "embeddings"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sb52lEewpUgg"
      },
      "source": [
        "embeddings.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v1R-_Y-jpUW8"
      },
      "source": [
        "attention_mask = tokens['attention_mask']\n",
        "attention_mask.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qom4XsGbpqlJ"
      },
      "source": [
        "mask = attention_mask.unsqueeze(-1).expand(embeddings.size()).float()\n",
        "mask.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BiAyKF-0puLD"
      },
      "source": [
        "mask"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NmNf2kdSpuC-"
      },
      "source": [
        "masked_embeddings = embeddings * mask\n",
        "masked_embeddings.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oAn82WNbpt7F"
      },
      "source": [
        "masked_embeddings"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uZLl0xKLptBR"
      },
      "source": [
        "summed = torch.sum(masked_embeddings, 1)\n",
        "summed.shape\n",
        "\n",
        "summed_mask = torch.clamp(mask.sum(1), min=1e-9)\n",
        "summed_mask.shape\n",
        "summed_mask"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OCCP0-JIqQme",
        "outputId": "089b9713-d27b-429d-93f1-97bb5e33c083"
      },
      "source": [
        "mean_pooled = summed / summed_mask\n",
        "mean_pooled\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "# convert from PyTorch tensor to numpy array\n",
        "mean_pooled = mean_pooled.detach().numpy()\n",
        "\n",
        "# calculate\n",
        "cosine_similarity(\n",
        "    [mean_pooled[0]],\n",
        "    mean_pooled[1:]\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.3308891 , 0.72192585, 0.17475495, 0.44709647, 0.55483633]],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H6mtZQLFPVK1"
      },
      "source": [
        "from nltk.corpus import gutenberg as gt\n",
        "nltk.download(\"gutenberg\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b9HDTtMyRT4U"
      },
      "source": [
        "print(gt.fileids()) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TXT803whWsfH"
      },
      "source": [
        "Emma = gt.raw(\"austen-emma.txt\")\n",
        "#print(Emma)\n",
        "Emma=Emma.format(r\"\\r\\n\\\\\")\n",
        "text_file=open('/content/drive/MyDrive/GoogleCollabWork/bs1.txt','w')\n",
        "text_file=text_file.write(Emma)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6RF91nEUxjOo"
      },
      "source": [
        "print(text_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KIhelG6hcJFb"
      },
      "source": [
        "Persuasion = gt.raw(\"austen-persuasion.txt\")\n",
        "print(Persuasion)\n",
        "Persuasion=Persuasion.format(r\"\\r\\n\\\\\")\n",
        "text_file=open('/content/drive/MyDrive/GoogleCollabWork/bs2.txt','w')\n",
        "text_file=text_file.write(Persuasion)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rSPNEzbbdyvu"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BuaX5biMllt3"
      },
      "source": [
        "# *BM25*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eC_lei87umDt"
      },
      "source": [
        "###STYLE1\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YibbA21Flz7y"
      },
      "source": [
        "!pip install rank_bm25\n",
        "!pip install git+https://www.git@github.com/dorianbrown/rank_bm25.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "411l-nEl5Ma6"
      },
      "source": [
        "from rank_bm25 import BM25Okapi\n",
        "\n",
        "corpus = file_docs\n",
        "\n",
        "tokenized_corpus = [doc.split(\" \") for doc in corpus]\n",
        "\n",
        "bm25 = BM25Okapi(tokenized_corpus)"
      ],
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QaWflVOP9-tS"
      },
      "source": [
        "print(tokenized_corpus)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dMnJ47Af5Rcs"
      },
      "source": [
        "query = ''.join(file2_docs)\n",
        "tokenized_query = query.split(\" \")\n",
        "doc_scores = bm25.get_scores(tokenized_query)\n",
        "print(doc_scores)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-pWNdGc180OM"
      },
      "source": [
        "bm25.get_top_n(tokenized_query, corpus, n=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vOZH5aHQ8r5m"
      },
      "source": [
        "query = \"In the men's rankings, Manish Kaushik is at the 18th spot in the 63kg weight category while Ashish Kumar and Satish Kumar are at the 9th spot in weight categories of 75kg and 91kg.\"\n",
        "tokenized_query = query.split(\" \")\n",
        "doc_scores = bm25.get_scores(tokenized_query)\n",
        "print(doc_scores)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZroXRHl4vNs8"
      },
      "source": [
        "!pip install tokenizers==0.8.0-rc1\n",
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M4mChUaxyEqT"
      },
      "source": [
        "import tokenize\n",
        "import tokenizers\n",
        "import nltk\n",
        "nltk.download('punkt') # one time execution\n",
        "import re"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4jtHic1ZDVZO"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xwHLieOFAQXo"
      },
      "source": [
        "import bm25Similarity"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ICC-RVaIBqRz"
      },
      "source": [
        "class TokenizedDocument(object):\n",
        "    def __init__(self, text):\n",
        "        self.sentences = []\n",
        "        self.text = text\n",
        "        self.preprocessed_split_indices = []\n",
        "        # this is only optionally kept\n",
        "        self.sentence_tokens_list = None\n",
        "        \n",
        "    def get_sentence_strings(self):\n",
        "        sentence_strings = []\n",
        "        for sentence in self.sentences:\n",
        "            if len(sentence) <= 0:\n",
        "                continue\n",
        "            #print(sentence)\n",
        "            # the 0th element is the start index, so let's get the start of the first and end of the last\n",
        "            start_idx = sentence[0][0]\n",
        "            end_idx = sentence[-1][1]\n",
        "            \n",
        "            sentence_string = self.text[start_idx : end_idx]\n",
        "            sentence_strings.append(sentence_string)\n",
        "        return sentence_strings\n",
        "        \n",
        "    def get_longest_sentence_string(self):\n",
        "        sentence_strings = self.get_sentence_strings()\n",
        "        longest_sentence_string = ''\n",
        "        for sentence_string in sentence_strings:\n",
        "            if len(sentence_string) > len(longest_sentence_string):\n",
        "                longest_sentence_string = sentence_string\n",
        "                \n",
        "        return longest_sentence_string\n",
        "        \n",
        "    def get_longest_sentence_token_indices(self):\n",
        "        sentences_tokens_sorted = self.sentences.copy()\n",
        "        sentences_tokens_sorted.sort(key = len, reverse = True)\n",
        "        return sentences_tokens_sorted"
      ],
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XXe5xkHZwjv3"
      },
      "source": [
        "textData1 = file_docs\n",
        "documents1 =  TokenizedDocument(textData1)"
      ],
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kYsW4DhowwqD"
      },
      "source": [
        "textData2 = file2_docs\n",
        "documents2 = TokenizedDocument(textData2)"
      ],
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L2UNs_VzCFEd"
      },
      "source": [
        "similarities = bm25Similarity(documents1,documents2,'DocumentLengthCorrection',1);"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}